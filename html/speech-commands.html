

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Speech Commands &mdash; OpenSeq2Seq 0.2 documentation</title>
  

  
  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  
  
  

  

  
  
    

  

  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/theme_override.css" type="text/css" />
  <link rel="stylesheet" href="_static/theme_override.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Multi-GPU and Distributed Training" href="distr-training.html" />
    <link rel="prev" title="Sentiment Analysis" href="sentiment-analysis.html" /> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> OpenSeq2Seq
          

          
            
            <img src="_static/logo.png" class="logo" alt="Logo"/>
          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="index.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation Instructions</a></li>
<li class="toctree-l1"><a class="reference internal" href="machine-translation.html">Machine Translation</a></li>
<li class="toctree-l1"><a class="reference internal" href="speech-recognition.html">Speech Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="speech-synthesis.html">Speech Synthesis</a></li>
<li class="toctree-l1"><a class="reference internal" href="language-model.html">Language Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="sentiment-analysis.html">Sentiment Analysis</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Speech Commands</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#dataset">Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="#models">Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#resnet-50">ResNet-50</a></li>
<li class="toctree-l3"><a class="reference internal" href="#jasper-10x3">Jasper-10x3</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#mixed-precision">Mixed Precision</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="distr-training.html">Multi-GPU and Distributed Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="mixed-precision.html">Mixed Precision Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="in-depth-tutorials.html">In-depth Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="interactive-infer-demos.html">Interactive Infer Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="api-docs/modules.html">API documentation</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">OpenSeq2Seq</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Speech Commands</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/speech-commands.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="speech-commands">
<span id="id1"></span><h1>Speech Commands<a class="headerlink" href="#speech-commands" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>The ability to recognize spoken commands with high accuracy can be useful in a variety of contexts. To this end, Google recently released the Speech Commands dataset (see <a class="reference external" href="https://arxiv.org/abs/1804.03209">paper</a>), which contains short audio clips of a fixed number of command words such as “stop”, “go”, “up”, “down”, etc spoken by a large number of speakers. To promote the use of the set, Google also hosted a <a class="reference external" href="https://www.kaggle.com/c/tensorflow-speech-recognition-challenge">Kaggle competition</a>, in which the winning team attained a multiclass accuracy of 91%.</p>
<p>We experimented with applying OpenSeq2Seq’s existing image classification models on mel spectrograms of the audio clips and found that they worked surprisingly well. Adding data augmentation further improved the results.</p>
</div>
<div class="section" id="dataset">
<h2>Dataset<a class="headerlink" href="#dataset" title="Permalink to this headline">¶</a></h2>
<p>Google released two versions of the dataset with the first version containing 65k samples over 30 classes and the second containing 110k samples over 35 classes. However, the Kaggle contest specification used only 10 of the provided classes, grouped the others as “unknown” and added “silence” for a total of 12 labels. We refer to these datasets as v1-12, v1-30 and v2, and have separate metrics for each version in order to compare to the different metrics used by other papers.</p>
<p>To preprocess a given version, we run <code class="docutils literal notranslate"><span class="pre">speech_commands_preprocessing.py</span></code> which first separates each class into training, validation and test sets with an 80-10-10 split. The script then balances the number of samples in each class by randomly duplicating samples in each class, which effectively grows the dataset since we apply random transformations to each sample in the data layer.</p>
<p>These samples are fed into the data layer, which randomly pitch shifts, time stretches and adds noise to each audio sample. These augmented samples are then converted into mel spectrograms and either randomly sliced or symmetrically padded with zeros until they are fixed dimension and square. These transformations are shown below.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/CommandAugmentations.png"><img alt="_images/CommandAugmentations.png" src="_images/CommandAugmentations.png" style="width: 924.8000000000001px; height: 472.0px;" /></a>
</div>
<p>To solve the bounded host bottleneck, the transformed images are cached during the first epoch in order to increase GPU utilization on subsequent epochs.</p>
</div>
<div class="section" id="models">
<h2>Models<a class="headerlink" href="#models" title="Permalink to this headline">¶</a></h2>
<p>The output of the data layer is a square image (128x128), which can then be fed into either ResNet or <a class="reference internal" href="speech-recognition/jasper.html"><span class="doc">Jasper</span></a>.</p>
<div class="section" id="resnet-50">
<h3>ResNet-50<a class="headerlink" href="#resnet-50" title="Permalink to this headline">¶</a></h3>
<p>The configuration file used for ResNet can be found <a class="reference external" href="https://github.com/NVIDIA/OpenSeq2Seq/blob/master/example_configs/image2label/resnet_commands.py">here</a>. The models were trained in mixed precision on 8 GPUs with a batch size of 32 over 100 epochs.</p>
<table border="1" class="colwidths-given docutils">
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Dataset</th>
<th class="head">Validation Accuracy</th>
<th class="head">Test Accuracy</th>
<th class="head">Checkpoint</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>v1-12</td>
<td>96.6%</td>
<td>96.6%</td>
<td><a class="reference external" href="https://drive.google.com/open?id=1RsaH95F2NVewBhJg1iC-WFiOaqgmeujK">link</a></td>
</tr>
<tr class="row-odd"><td>v1-30</td>
<td>97.5%</td>
<td>97.3%</td>
<td><a class="reference external" href="https://drive.google.com/open?id=19JDOdqLa9knH2Qryvl6gf4wkWc6__Yqw">link</a></td>
</tr>
<tr class="row-even"><td>v2</td>
<td>95.7%</td>
<td>95.9%</td>
<td><a class="reference external" href="https://drive.google.com/open?id=1r_Ksr1UBPG2ZUe4eja_fYxVReRqawBkg">link</a></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="jasper-10x3">
<h3>Jasper-10x3<a class="headerlink" href="#jasper-10x3" title="Permalink to this headline">¶</a></h3>
<p>The configuration file used for Jasper can be found <a class="reference external" href="https://github.com/NVIDIA/OpenSeq2Seq/blob/master/example_configs/speech2text/jasper_commands.py">here</a>. The models were trained in mixed precision on 8 GPUs with a batch size of 64 over 200 epochs.</p>
<table border="1" class="colwidths-given docutils">
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Dataset</th>
<th class="head">Validation Accuracy</th>
<th class="head">Test Accuracy</th>
<th class="head">Checkpoint</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>v1-12</td>
<td>97.1%</td>
<td>96.2%</td>
<td><a class="reference external" href="https://drive.google.com/open?id=1Kwl1CtI1STh7Lbza3WASJPkXLeWITo7f">link</a></td>
</tr>
<tr class="row-odd"><td>v1-30</td>
<td>97.5%</td>
<td>97.3%</td>
<td><a class="reference external" href="https://drive.google.com/open?id=1MxR5ptUl1fjUJoOrqUnsHdBnc5_cClKc">link</a></td>
</tr>
<tr class="row-even"><td>v2</td>
<td>95.5%</td>
<td>95.1%</td>
<td><a class="reference external" href="https://drive.google.com/open?id=1s6e4YqkgsGdJfCnByDJ0tnQAc_RpiKp0">link</a></td>
</tr>
</tbody>
</table>
<p>To use a different dataset, the only change required is to the <code class="docutils literal notranslate"><span class="pre">dataset_version</span></code> parameter, which should be set to one of <code class="docutils literal notranslate"><span class="pre">v1-12</span></code>, <code class="docutils literal notranslate"><span class="pre">v1-30</span></code> or <code class="docutils literal notranslate"><span class="pre">v2</span></code>.</p>
</div>
</div>
<div class="section" id="mixed-precision">
<h2>Mixed Precision<a class="headerlink" href="#mixed-precision" title="Permalink to this headline">¶</a></h2>
<p>We found that the model trains just as well in mixed precision, attaining the same results with half the GPU memory. A constant loss scaling of 512.0 was used for ResNet, which saw a 12% speedup from float using the same batch size. Similarly, backoff loss scaling was used for Jasper, which saw a 15% speedup from float.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="distr-training.html" class="btn btn-neutral float-right" title="Multi-GPU and Distributed Training" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="sentiment-analysis.html" class="btn btn-neutral" title="Sentiment Analysis" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, NVIDIA.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.2',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  <script type="text/javascript">
      jQuery(function () {
          
          SphinxRtdTheme.Navigation.enableSticky();
          
      });
  </script>  
  <style>
    /* Sidebar header (and topbar for mobile) */
    .wy-side-nav-search, .wy-nav-top {
      background: #64d81c;
    }
    .wy-side-nav-search > div.version {
      color: #ffffff;
    }
    .wy-side-nav-search > img {
      max-width: 150px;
    }
    .wy-side-nav-search > a {
      font-size: 23px;
    }
  </style>


</body>
</html>