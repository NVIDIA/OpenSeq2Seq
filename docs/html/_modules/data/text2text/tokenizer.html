

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>data.text2text.tokenizer &mdash; OpenSeq2Seq 0.2 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../../../_static/favicon.ico"/>
  
  
  

  

  
  
    

  

  
    <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/theme_override.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/theme_override.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../../index.html" class="icon icon-home"> OpenSeq2Seq
          

          
            
            <img src="../../../_static/logo.png" class="logo" alt="Logo"/>
          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../index.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../installation.html">Installation instructions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../machine-translation.html">Machine Translation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../speech-recognition.html">Speech Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../speech-synthesis.html">Speech Synthesis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distr-training.html">Distributed training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mixed-precision.html">Mixed precision training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../in-depth-tutorials.html">In-depth tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../interactive-infer-demos.html">Interactive Infer Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api-docs/modules.html">API documentation</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">OpenSeq2Seq</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
      <li>data.text2text.tokenizer</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for data.text2text.tokenizer</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2018 MLBenchmark Group. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ==============================================================================</span>
<span class="sd">&quot;&quot;&quot;Defines Subtokenizer class to encode and decode strings.&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">unicodedata</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">six</span>
<span class="kn">from</span> <span class="nn">six.moves</span> <span class="k">import</span> <span class="n">xrange</span>  <span class="c1"># pylint: disable=redefined-builtin</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="n">PAD</span> <span class="o">=</span> <span class="s2">&quot;&lt;pad&gt;&quot;</span>
<span class="n">PAD_ID</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">EOS</span> <span class="o">=</span> <span class="s2">&quot;&lt;EOS&gt;&quot;</span>
<span class="n">EOS_ID</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">RESERVED_TOKENS</span> <span class="o">=</span> <span class="p">[</span><span class="n">PAD</span><span class="p">,</span> <span class="n">EOS</span><span class="p">]</span>

<span class="c1"># Set of characters that will be used in the function _escape_token() (see func</span>
<span class="c1"># docstring for more details).</span>
<span class="c1"># This set is added to the alphabet list to ensure that all escaped tokens can</span>
<span class="c1"># be encoded.</span>
<span class="n">_ESCAPE_CHARS</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="sa">u</span><span class="s2">&quot;</span><span class="se">\\</span><span class="s2">_u;0123456789&quot;</span><span class="p">)</span>
<span class="c1"># Regex for the function _unescape_token(), the inverse of _escape_token().</span>
<span class="c1"># This is used to find &quot;\u&quot;, &quot;\\&quot;, and &quot;\###;&quot; substrings in the token.</span>
<span class="n">_UNESCAPE_REGEX</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;</span><span class="se">\\</span><span class="s2">u|</span><span class="se">\\\\</span><span class="s2">|</span><span class="se">\\</span><span class="s2">([0-9]+);&quot;</span><span class="p">)</span>

<span class="n">_UNDEFINED_UNICODE</span> <span class="o">=</span> <span class="sa">u</span><span class="s2">&quot;</span><span class="se">\u3013</span><span class="s2">&quot;</span>

<span class="c1"># Set contains all letter and number characters.</span>
<span class="n">_ALPHANUMERIC_CHAR_SET</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span>
    <span class="n">six</span><span class="o">.</span><span class="n">unichr</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">maxunicode</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">unicodedata</span><span class="o">.</span><span class="n">category</span><span class="p">(</span><span class="n">six</span><span class="o">.</span><span class="n">unichr</span><span class="p">(</span><span class="n">i</span><span class="p">))</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;L&quot;</span><span class="p">)</span> <span class="ow">or</span>
        <span class="n">unicodedata</span><span class="o">.</span><span class="n">category</span><span class="p">(</span><span class="n">six</span><span class="o">.</span><span class="n">unichr</span><span class="p">(</span><span class="n">i</span><span class="p">))</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;N&quot;</span><span class="p">)))</span>

<span class="c1"># min_count is the minimum number of times a subtoken must appear in the data</span>
<span class="c1"># before before it is added to the vocabulary. The value is found using binary</span>
<span class="c1"># search to obtain the target vocabulary size.</span>
<span class="n">_MIN_MIN_COUNT</span> <span class="o">=</span> <span class="mi">1</span>     <span class="c1"># min value to use when binary searching for min_count</span>
<span class="n">_MAX_MIN_COUNT</span> <span class="o">=</span> <span class="mi">1000</span>  <span class="c1"># max value to use when binary searching for min_count</span>


<div class="viewcode-block" id="Subtokenizer"><a class="viewcode-back" href="../../../api-docs/data.text2text.html#data.text2text.tokenizer.Subtokenizer">[docs]</a><span class="k">class</span> <span class="nc">Subtokenizer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Encodes and decodes strings to/from integer IDs.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="Subtokenizer.__init__"><a class="viewcode-back" href="../../../api-docs/data.text2text.html#data.text2text.tokenizer.Subtokenizer.__init__">[docs]</a>  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_file</span><span class="p">,</span> <span class="n">reserved_tokens</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initializes class, creating a vocab file if data_files is provided.&quot;&quot;&quot;</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Initializing Subtokenizer from file </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span> <span class="n">vocab_file</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">reserved_tokens</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">reserved_tokens</span> <span class="o">=</span> <span class="n">RESERVED_TOKENS</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">subtoken_list</span> <span class="o">=</span> <span class="n">_load_vocab_file</span><span class="p">(</span><span class="n">vocab_file</span><span class="p">,</span> <span class="n">reserved_tokens</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">alphabet</span> <span class="o">=</span> <span class="n">_generate_alphabet_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">subtoken_list</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">subtoken_to_id_dict</span> <span class="o">=</span> <span class="n">_list_to_index_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">subtoken_list</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">max_subtoken_length</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">subtoken</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">subtoken_list</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">max_subtoken_length</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_subtoken_length</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">subtoken</span><span class="p">))</span>

    <span class="c1"># Create cache to speed up subtokenization</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_cache_size</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="mi">20</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span> <span class="o">=</span> <span class="p">[(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cache_size</span></div>

<div class="viewcode-block" id="Subtokenizer.init_from_files"><a class="viewcode-back" href="../../../api-docs/data.text2text.html#data.text2text.tokenizer.Subtokenizer.init_from_files">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">init_from_files</span><span class="p">(</span>
      <span class="n">vocab_file</span><span class="p">,</span> <span class="n">files</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="p">,</span> <span class="n">threshold</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">file_byte_limit</span><span class="o">=</span><span class="mf">1e6</span><span class="p">,</span> <span class="n">reserved_tokens</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Create subtoken vocabulary based on files, and save vocab to file.</span>

<span class="sd">    Args:</span>
<span class="sd">      vocab_file: String name of vocab file to store subtoken vocabulary.</span>
<span class="sd">      files: List of file paths that will be used to generate vocabulary.</span>
<span class="sd">      target_vocab_size: target vocabulary size to generate.</span>
<span class="sd">      threshold: int threshold of vocabulary size to accept.</span>
<span class="sd">      min_count: int minimum count to use for generating the vocabulary. The min</span>
<span class="sd">        count is the minimum number of times a subtoken should appear in the</span>
<span class="sd">        files before it is added to the vocabulary. If set to none, this value</span>
<span class="sd">        is found using binary search.</span>
<span class="sd">      file_byte_limit: (Default 1e6) Maximum number of bytes of sample text that</span>
<span class="sd">        will be drawn from the files.</span>
<span class="sd">      reserved_tokens: List of string tokens that are guaranteed to be at the</span>
<span class="sd">        beginning of the subtoken vocabulary list.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Subtokenizer object</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">reserved_tokens</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">reserved_tokens</span> <span class="o">=</span> <span class="n">RESERVED_TOKENS</span>

    <span class="k">if</span> <span class="n">tf</span><span class="o">.</span><span class="n">gfile</span><span class="o">.</span><span class="n">Exists</span><span class="p">(</span><span class="n">vocab_file</span><span class="p">):</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Vocab file already exists (</span><span class="si">%s</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="n">vocab_file</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Begin steps to create subtoken vocabulary...&quot;</span><span class="p">)</span>
      <span class="n">token_counts</span> <span class="o">=</span> <span class="n">_count_tokens</span><span class="p">(</span><span class="n">files</span><span class="p">,</span> <span class="n">file_byte_limit</span><span class="p">)</span>
      <span class="n">alphabet</span> <span class="o">=</span> <span class="n">_generate_alphabet_dict</span><span class="p">(</span><span class="n">token_counts</span><span class="p">)</span>
      <span class="n">subtoken_list</span> <span class="o">=</span> <span class="n">_generate_subtokens_with_target_vocab_size</span><span class="p">(</span>
          <span class="n">token_counts</span><span class="p">,</span> <span class="n">alphabet</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="p">,</span> <span class="n">threshold</span><span class="p">,</span> <span class="n">min_count</span><span class="p">,</span>
          <span class="n">reserved_tokens</span><span class="p">)</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Generated vocabulary with </span><span class="si">%d</span><span class="s2"> subtokens.&quot;</span> <span class="o">%</span>
                      <span class="nb">len</span><span class="p">(</span><span class="n">subtoken_list</span><span class="p">))</span>
      <span class="n">_save_vocab_file</span><span class="p">(</span><span class="n">vocab_file</span><span class="p">,</span> <span class="n">subtoken_list</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Subtokenizer</span><span class="p">(</span><span class="n">vocab_file</span><span class="p">)</span></div>

<div class="viewcode-block" id="Subtokenizer.encode"><a class="viewcode-back" href="../../../api-docs/data.text2text.html#data.text2text.tokenizer.Subtokenizer.encode">[docs]</a>  <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">raw_string</span><span class="p">,</span> <span class="n">add_eos</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Encodes a string into a list of int subtoken ids.&quot;&quot;&quot;</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">_split_string_to_tokens</span><span class="p">(</span><span class="n">_native_to_unicode</span><span class="p">(</span><span class="n">raw_string</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
      <span class="n">ret</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_token_to_subtoken_ids</span><span class="p">(</span><span class="n">token</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">add_eos</span><span class="p">:</span>
      <span class="n">ret</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">EOS_ID</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span></div>

<div class="viewcode-block" id="Subtokenizer._token_to_subtoken_ids"><a class="viewcode-back" href="../../../api-docs/data.text2text.html#data.text2text.tokenizer.Subtokenizer._token_to_subtoken_ids">[docs]</a>  <span class="k">def</span> <span class="nf">_token_to_subtoken_ids</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">token</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Encode a single token into a list of subtoken ids.&quot;&quot;&quot;</span>
    <span class="n">cache_location</span> <span class="o">=</span> <span class="nb">hash</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cache_size</span>
    <span class="n">cache_key</span><span class="p">,</span> <span class="n">cache_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">[</span><span class="n">cache_location</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">cache_key</span> <span class="o">==</span> <span class="n">token</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">cache_value</span>

    <span class="n">ret</span> <span class="o">=</span> <span class="n">_split_token_to_subtokens</span><span class="p">(</span>
        <span class="n">_escape_token</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">alphabet</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">subtoken_to_id_dict</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_subtoken_length</span><span class="p">)</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">subtoken_to_id_dict</span><span class="p">[</span><span class="n">subtoken_id</span><span class="p">]</span> <span class="k">for</span> <span class="n">subtoken_id</span> <span class="ow">in</span> <span class="n">ret</span><span class="p">]</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">[</span><span class="n">cache_location</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">ret</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span></div>

<div class="viewcode-block" id="Subtokenizer.decode"><a class="viewcode-back" href="../../../api-docs/data.text2text.html#data.text2text.tokenizer.Subtokenizer.decode">[docs]</a>  <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">subtokens</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Converts list of int subtokens ids into a string.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">subtokens</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
      <span class="c1"># Note that list(subtokens) converts subtokens to a python list, but the</span>
      <span class="c1"># items remain as np.int32. This converts both the array and its items.</span>
      <span class="n">subtokens</span> <span class="o">=</span> <span class="n">subtokens</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">subtokens</span><span class="p">:</span>
      <span class="k">return</span> <span class="s2">&quot;&quot;</span>

    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">subtokens</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">subtokens</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">int</span><span class="p">),</span> <span class="p">(</span>
        <span class="s2">&quot;Subtokens argument passed into decode() must be a list of integers.&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">_unicode_to_native</span><span class="p">(</span>
        <span class="n">join_tokens_to_string</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_subtoken_ids_to_tokens</span><span class="p">(</span><span class="n">subtokens</span><span class="p">)))</span></div>

<div class="viewcode-block" id="Subtokenizer._subtoken_ids_to_tokens"><a class="viewcode-back" href="../../../api-docs/data.text2text.html#data.text2text.tokenizer.Subtokenizer._subtoken_ids_to_tokens">[docs]</a>  <span class="k">def</span> <span class="nf">_subtoken_ids_to_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">subtokens</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Convert list of int subtoken ids to a list of string tokens.&quot;&quot;&quot;</span>
    <span class="n">escaped_tokens</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">subtoken_list</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">subtokens</span>
        <span class="k">if</span> <span class="n">s</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">subtoken_list</span><span class="p">)])</span>
    <span class="n">escaped_tokens</span> <span class="o">=</span> <span class="n">escaped_tokens</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">)</span>

    <span class="c1"># All tokens in the vocabulary list have been escaped (see _escape_token())</span>
    <span class="c1"># so each token must be unescaped when decoding.</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">escaped_tokens</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">token</span><span class="p">:</span>
        <span class="n">ret</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">unescape_token</span><span class="p">(</span><span class="n">token</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">ret</span></div></div>


<div class="viewcode-block" id="_save_vocab_file"><a class="viewcode-back" href="../../../api-docs/data.text2text.html#data.text2text.tokenizer._save_vocab_file">[docs]</a><span class="k">def</span> <span class="nf">_save_vocab_file</span><span class="p">(</span><span class="n">vocab_file</span><span class="p">,</span> <span class="n">subtoken_list</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Save subtokens to file.&quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">gfile</span><span class="o">.</span><span class="n">Open</span><span class="p">(</span><span class="n">vocab_file</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">subtoken</span> <span class="ow">in</span> <span class="n">subtoken_list</span><span class="p">:</span>
      <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">&quot;&#39;</span><span class="si">%s</span><span class="s2">&#39;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">_unicode_to_native</span><span class="p">(</span><span class="n">subtoken</span><span class="p">))</span></div>


<div class="viewcode-block" id="_load_vocab_file"><a class="viewcode-back" href="../../../api-docs/data.text2text.html#data.text2text.tokenizer._load_vocab_file">[docs]</a><span class="k">def</span> <span class="nf">_load_vocab_file</span><span class="p">(</span><span class="n">vocab_file</span><span class="p">,</span> <span class="n">reserved_tokens</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Load vocabulary while ensuring reserved tokens are at the top.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">reserved_tokens</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">reserved_tokens</span> <span class="o">=</span> <span class="n">RESERVED_TOKENS</span>

  <span class="n">subtoken_list</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">gfile</span><span class="o">.</span><span class="n">Open</span><span class="p">(</span><span class="n">vocab_file</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
      <span class="n">subtoken</span> <span class="o">=</span> <span class="n">_native_to_unicode</span><span class="p">(</span><span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>
      <span class="n">subtoken</span> <span class="o">=</span> <span class="n">subtoken</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># Remove surrounding single-quotes</span>
      <span class="k">if</span> <span class="n">subtoken</span> <span class="ow">in</span> <span class="n">reserved_tokens</span><span class="p">:</span>
        <span class="k">continue</span>
      <span class="n">subtoken_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">_native_to_unicode</span><span class="p">(</span><span class="n">subtoken</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">reserved_tokens</span> <span class="o">+</span> <span class="n">subtoken_list</span></div>


<div class="viewcode-block" id="_native_to_unicode"><a class="viewcode-back" href="../../../api-docs/data.text2text.html#data.text2text.tokenizer._native_to_unicode">[docs]</a><span class="k">def</span> <span class="nf">_native_to_unicode</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Convert string to unicode (required in Python 2).&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">six</span><span class="o">.</span><span class="n">PY2</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">s</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">unicode</span><span class="p">)</span> <span class="k">else</span> <span class="n">s</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">s</span></div>


<div class="viewcode-block" id="_unicode_to_native"><a class="viewcode-back" href="../../../api-docs/data.text2text.html#data.text2text.tokenizer._unicode_to_native">[docs]</a><span class="k">def</span> <span class="nf">_unicode_to_native</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Convert string from unicode to native format (required in Python 2).&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">six</span><span class="o">.</span><span class="n">PY2</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">s</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">unicode</span><span class="p">)</span> <span class="k">else</span> <span class="n">s</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">s</span></div>


<div class="viewcode-block" id="_split_string_to_tokens"><a class="viewcode-back" href="../../../api-docs/data.text2text.html#data.text2text.tokenizer._split_string_to_tokens">[docs]</a><span class="k">def</span> <span class="nf">_split_string_to_tokens</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Splits text to a list of string tokens.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">text</span><span class="p">:</span>
    <span class="k">return</span> <span class="p">[]</span>
  <span class="n">ret</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">token_start</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="c1"># Classify each character in the input string</span>
  <span class="n">is_alnum</span> <span class="o">=</span> <span class="p">[</span><span class="n">c</span> <span class="ow">in</span> <span class="n">_ALPHANUMERIC_CHAR_SET</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">text</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">pos</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)):</span>
    <span class="k">if</span> <span class="n">is_alnum</span><span class="p">[</span><span class="n">pos</span><span class="p">]</span> <span class="o">!=</span> <span class="n">is_alnum</span><span class="p">[</span><span class="n">pos</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]:</span>
      <span class="n">token</span> <span class="o">=</span> <span class="n">text</span><span class="p">[</span><span class="n">token_start</span><span class="p">:</span><span class="n">pos</span><span class="p">]</span>
      <span class="k">if</span> <span class="n">token</span> <span class="o">!=</span> <span class="sa">u</span><span class="s2">&quot; &quot;</span> <span class="ow">or</span> <span class="n">token_start</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">ret</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
      <span class="n">token_start</span> <span class="o">=</span> <span class="n">pos</span>
  <span class="n">final_token</span> <span class="o">=</span> <span class="n">text</span><span class="p">[</span><span class="n">token_start</span><span class="p">:]</span>
  <span class="n">ret</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">final_token</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">ret</span></div>


<div class="viewcode-block" id="join_tokens_to_string"><a class="viewcode-back" href="../../../api-docs/data.text2text.html#data.text2text.tokenizer.join_tokens_to_string">[docs]</a><span class="k">def</span> <span class="nf">join_tokens_to_string</span><span class="p">(</span><span class="n">tokens</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Join a list of string tokens into a single string.&quot;&quot;&quot;</span>
  <span class="n">token_is_alnum</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">in</span> <span class="n">_ALPHANUMERIC_CHAR_SET</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
  <span class="n">ret</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tokens</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">token_is_alnum</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="ow">and</span> <span class="n">token_is_alnum</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
      <span class="n">ret</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">u</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
    <span class="n">ret</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
  <span class="k">return</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span></div>


<div class="viewcode-block" id="_escape_token"><a class="viewcode-back" href="../../../api-docs/data.text2text.html#data.text2text.tokenizer._escape_token">[docs]</a><span class="k">def</span> <span class="nf">_escape_token</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">alphabet</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Replace characters that aren&#39;t in the alphabet and append &quot;_&quot; to token.</span>

<span class="sd">  Apply three transformations to the token:</span>
<span class="sd">    1. Replace underline character &quot;_&quot; with &quot;\u&quot;, and backslash &quot;\&quot; with &quot;\\&quot;.</span>
<span class="sd">    2. Replace characters outside of the alphabet with &quot;\###;&quot;, where ### is the</span>
<span class="sd">       character&#39;s Unicode code point.</span>
<span class="sd">    3. Appends &quot;_&quot; to mark the end of a token.</span>

<span class="sd">  Args:</span>
<span class="sd">    token: unicode string to be escaped</span>
<span class="sd">    alphabet: list of all known characters</span>

<span class="sd">  Returns:</span>
<span class="sd">    escaped string</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">token</span> <span class="o">=</span> <span class="n">token</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="sa">u</span><span class="s2">&quot;</span><span class="se">\\</span><span class="s2">&quot;</span><span class="p">,</span> <span class="sa">u</span><span class="s2">&quot;</span><span class="se">\\\\</span><span class="s2">&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="sa">u</span><span class="s2">&quot;_&quot;</span><span class="p">,</span> <span class="sa">u</span><span class="s2">&quot;</span><span class="se">\\</span><span class="s2">u&quot;</span><span class="p">)</span>
  <span class="n">ret</span> <span class="o">=</span> <span class="p">[</span><span class="n">c</span> <span class="k">if</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">alphabet</span> <span class="ow">and</span> <span class="n">c</span> <span class="o">!=</span> <span class="sa">u</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="k">else</span> <span class="sa">r</span><span class="s2">&quot;\</span><span class="si">%d</span><span class="s2">;&quot;</span> <span class="o">%</span> <span class="nb">ord</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">token</span><span class="p">]</span>
  <span class="k">return</span> <span class="sa">u</span><span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;_&quot;</span></div>


<div class="viewcode-block" id="unescape_token"><a class="viewcode-back" href="../../../api-docs/data.text2text.html#data.text2text.tokenizer.unescape_token">[docs]</a><span class="k">def</span> <span class="nf">unescape_token</span><span class="p">(</span><span class="n">token</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Replaces escaped characters in the token with their unescaped versions.</span>

<span class="sd">  Applies inverse transformations as _escape_token():</span>
<span class="sd">    1. Replace &quot;\u&quot; with &quot;_&quot;, and &quot;\\&quot; with &quot;\&quot;.</span>
<span class="sd">    2. Replace &quot;\###;&quot; with the unicode character the ### refers to.</span>

<span class="sd">  Args:</span>
<span class="sd">    token: escaped string</span>

<span class="sd">  Returns:</span>
<span class="sd">    unescaped string</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">match</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns replacement string for matched object.</span>

<span class="sd">    Matched objects contain one of the strings that matches the regex pattern:</span>
<span class="sd">      r&quot;\\u|\\\\|\\([0-9]+);&quot;</span>
<span class="sd">    The strings can be &#39;\u&#39;, &#39;\\&#39;, or &#39;\###;&#39; (### is any digit number).</span>

<span class="sd">    m.group(0) refers to the entire matched string (&#39;\u&#39;, &#39;\\&#39;, or &#39;\###;&#39;).</span>
<span class="sd">    m.group(1) refers to the first parenthesized subgroup (&#39;###&#39;).</span>

<span class="sd">    m.group(0) exists for all match objects, while m.group(1) exists only for</span>
<span class="sd">    the string &#39;\###;&#39;.</span>

<span class="sd">    This function looks to see if m.group(1) exists. If it doesn&#39;t, then the</span>
<span class="sd">    matched string must be &#39;\u&#39; or &#39;\\&#39; . In this case, the corresponding</span>
<span class="sd">    replacement (&#39;_&#39; and &#39;\&#39;) are returned. Note that in python, a single</span>
<span class="sd">    backslash is written as &#39;\\&#39;, and double backslash as &#39;\\\\&#39;.</span>

<span class="sd">    If m.goup(1) exists, then use the integer in m.group(1) to return a</span>
<span class="sd">    unicode character.</span>

<span class="sd">    Args:</span>
<span class="sd">      m: match object</span>

<span class="sd">    Returns:</span>
<span class="sd">      String to replace matched object with.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Check if the matched strings are &#39;\u&#39; or &#39;\\&#39;.</span>
    <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="sa">u</span><span class="s2">&quot;_&quot;</span> <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="sa">u</span><span class="s2">&quot;</span><span class="se">\\</span><span class="s2">u&quot;</span> <span class="k">else</span> <span class="sa">u</span><span class="s2">&quot;</span><span class="se">\\</span><span class="s2">&quot;</span>

    <span class="c1"># If m.group(1) exists, try and return unicode character.</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">six</span><span class="o">.</span><span class="n">unichr</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">1</span><span class="p">)))</span>
    <span class="k">except</span> <span class="p">(</span><span class="ne">ValueError</span><span class="p">,</span> <span class="ne">OverflowError</span><span class="p">)</span> <span class="k">as</span> <span class="n">_</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">_UNDEFINED_UNICODE</span>

  <span class="c1"># Use match function to replace escaped substrings in the token.</span>
  <span class="k">return</span> <span class="n">_UNESCAPE_REGEX</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">match</span><span class="p">,</span> <span class="n">token</span><span class="p">)</span></div>


<div class="viewcode-block" id="_count_tokens"><a class="viewcode-back" href="../../../api-docs/data.text2text.html#data.text2text.tokenizer._count_tokens">[docs]</a><span class="k">def</span> <span class="nf">_count_tokens</span><span class="p">(</span><span class="n">files</span><span class="p">,</span> <span class="n">file_byte_limit</span><span class="o">=</span><span class="mf">1e6</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Return token counts of words in the files.</span>

<span class="sd">  Samples file_byte_limit bytes from each file, and counts the words that appear</span>
<span class="sd">  in the samples. The samples are semi-evenly distributed across the file.</span>

<span class="sd">  Args:</span>
<span class="sd">    files: List of filepaths</span>
<span class="sd">    file_byte_limit: Max number of bytes that will be read from each file.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Dictionary mapping tokens to the number of times they appear in the sampled</span>
<span class="sd">    lines from the files.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">token_counts</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

  <span class="k">for</span> <span class="n">filepath</span> <span class="ow">in</span> <span class="n">files</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">gfile</span><span class="o">.</span><span class="n">Open</span><span class="p">(</span><span class="n">filepath</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">reader</span><span class="p">:</span>
      <span class="n">file_byte_budget</span> <span class="o">=</span> <span class="n">file_byte_limit</span>
      <span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
      <span class="n">lines_to_skip</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">reader</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="n">file_byte_budget</span> <span class="o">*</span> <span class="mi">2</span><span class="p">))</span>
      <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">reader</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">counter</span> <span class="o">&lt;</span> <span class="n">lines_to_skip</span><span class="p">:</span>
          <span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="k">if</span> <span class="n">file_byte_budget</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">break</span>
          <span class="n">line</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
          <span class="n">file_byte_budget</span> <span class="o">-=</span> <span class="nb">len</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
          <span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>

          <span class="c1"># Add words to token counts</span>
          <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">_split_string_to_tokens</span><span class="p">(</span><span class="n">_native_to_unicode</span><span class="p">(</span><span class="n">line</span><span class="p">)):</span>
            <span class="n">token_counts</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
  <span class="k">return</span> <span class="n">token_counts</span></div>


<div class="viewcode-block" id="_list_to_index_dict"><a class="viewcode-back" href="../../../api-docs/data.text2text.html#data.text2text.tokenizer._list_to_index_dict">[docs]</a><span class="k">def</span> <span class="nf">_list_to_index_dict</span><span class="p">(</span><span class="n">lst</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Create dictionary mapping list items to their indices in the list.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="p">{</span><span class="n">item</span><span class="p">:</span> <span class="n">n</span> <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">lst</span><span class="p">)}</span></div>


<div class="viewcode-block" id="_split_token_to_subtokens"><a class="viewcode-back" href="../../../api-docs/data.text2text.html#data.text2text.tokenizer._split_token_to_subtokens">[docs]</a><span class="k">def</span> <span class="nf">_split_token_to_subtokens</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">subtoken_dict</span><span class="p">,</span> <span class="n">max_subtoken_length</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Splits a token into subtokens defined in the subtoken dict.&quot;&quot;&quot;</span>
  <span class="n">ret</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">token_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
  <span class="k">while</span> <span class="n">start</span> <span class="o">&lt;</span> <span class="n">token_len</span><span class="p">:</span>
    <span class="c1"># Find the longest subtoken, so iterate backwards.</span>
    <span class="k">for</span> <span class="n">end</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">token_len</span><span class="p">,</span> <span class="n">start</span> <span class="o">+</span> <span class="n">max_subtoken_length</span><span class="p">),</span> <span class="n">start</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
      <span class="n">subtoken</span> <span class="o">=</span> <span class="n">token</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span>
      <span class="k">if</span> <span class="n">subtoken</span> <span class="ow">in</span> <span class="n">subtoken_dict</span><span class="p">:</span>
        <span class="n">ret</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">subtoken</span><span class="p">)</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">end</span>
        <span class="k">break</span>
    <span class="k">else</span><span class="p">:</span>  <span class="c1"># Did not break</span>
      <span class="c1"># If there is no possible encoding of the escaped token then one of the</span>
      <span class="c1"># characters in the token is not in the alphabet. This should be</span>
      <span class="c1"># impossible and would be indicative of a bug.</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Was unable to split token </span><span class="se">\&quot;</span><span class="si">%s</span><span class="se">\&quot;</span><span class="s2"> into subtokens.&quot;</span> <span class="o">%</span>
                       <span class="n">token</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">ret</span></div>


<div class="viewcode-block" id="_generate_subtokens_with_target_vocab_size"><a class="viewcode-back" href="../../../api-docs/data.text2text.html#data.text2text.tokenizer._generate_subtokens_with_target_vocab_size">[docs]</a><span class="k">def</span> <span class="nf">_generate_subtokens_with_target_vocab_size</span><span class="p">(</span>
    <span class="n">token_counts</span><span class="p">,</span> <span class="n">alphabet</span><span class="p">,</span> <span class="n">target_size</span><span class="p">,</span> <span class="n">threshold</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">reserved_tokens</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Generate subtoken vocabulary close to the target size.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">reserved_tokens</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">reserved_tokens</span> <span class="o">=</span> <span class="n">RESERVED_TOKENS</span>

  <span class="k">if</span> <span class="n">min_count</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Using min_count=</span><span class="si">%d</span><span class="s2"> to generate vocab with target size </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span>
                    <span class="p">(</span><span class="n">min_count</span><span class="p">,</span> <span class="n">target_size</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">_generate_subtokens</span><span class="p">(</span>
        <span class="n">token_counts</span><span class="p">,</span> <span class="n">alphabet</span><span class="p">,</span> <span class="n">min_count</span><span class="p">,</span> <span class="n">reserved_tokens</span><span class="o">=</span><span class="n">reserved_tokens</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">bisect</span><span class="p">(</span><span class="n">min_val</span><span class="p">,</span> <span class="n">max_val</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Recursive function to binary search for subtoken vocabulary.&quot;&quot;&quot;</span>
    <span class="n">cur_count</span> <span class="o">=</span> <span class="p">(</span><span class="n">min_val</span> <span class="o">+</span> <span class="n">max_val</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Binary search: trying min_count=</span><span class="si">%d</span><span class="s2"> (</span><span class="si">%d</span><span class="s2"> </span><span class="si">%d</span><span class="s2">)&quot;</span> <span class="o">%</span>
                    <span class="p">(</span><span class="n">cur_count</span><span class="p">,</span> <span class="n">min_val</span><span class="p">,</span> <span class="n">max_val</span><span class="p">))</span>
    <span class="n">subtoken_list</span> <span class="o">=</span> <span class="n">_generate_subtokens</span><span class="p">(</span>
        <span class="n">token_counts</span><span class="p">,</span> <span class="n">alphabet</span><span class="p">,</span> <span class="n">cur_count</span><span class="p">,</span> <span class="n">reserved_tokens</span><span class="o">=</span><span class="n">reserved_tokens</span><span class="p">)</span>

    <span class="n">val</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">subtoken_list</span><span class="p">)</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Binary search: min_count=</span><span class="si">%d</span><span class="s2"> resulted in </span><span class="si">%d</span><span class="s2"> tokens&quot;</span> <span class="o">%</span>
                    <span class="p">(</span><span class="n">cur_count</span><span class="p">,</span> <span class="n">val</span><span class="p">))</span>

    <span class="n">within_threshold</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">val</span> <span class="o">-</span> <span class="n">target_size</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">threshold</span>
    <span class="k">if</span> <span class="n">within_threshold</span> <span class="ow">or</span> <span class="n">min_val</span> <span class="o">&gt;=</span> <span class="n">max_val</span> <span class="ow">or</span> <span class="n">cur_count</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">subtoken_list</span>
    <span class="k">if</span> <span class="n">val</span> <span class="o">&gt;</span> <span class="n">target_size</span><span class="p">:</span>
      <span class="n">other_subtoken_list</span> <span class="o">=</span> <span class="n">bisect</span><span class="p">(</span><span class="n">cur_count</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">max_val</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">other_subtoken_list</span> <span class="o">=</span> <span class="n">bisect</span><span class="p">(</span><span class="n">min_val</span><span class="p">,</span> <span class="n">cur_count</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Return vocabulary dictionary with the closest number of tokens.</span>
    <span class="n">other_val</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">other_subtoken_list</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">other_val</span> <span class="o">-</span> <span class="n">target_size</span><span class="p">)</span> <span class="o">&lt;</span> <span class="nb">abs</span><span class="p">(</span><span class="n">val</span> <span class="o">-</span> <span class="n">target_size</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">other_subtoken_list</span>
    <span class="k">return</span> <span class="n">subtoken_list</span>

  <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Finding best min_count to get target size of </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span>
                  <span class="n">target_size</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">bisect</span><span class="p">(</span><span class="n">_MIN_MIN_COUNT</span><span class="p">,</span> <span class="n">_MAX_MIN_COUNT</span><span class="p">)</span></div>


<div class="viewcode-block" id="_generate_alphabet_dict"><a class="viewcode-back" href="../../../api-docs/data.text2text.html#data.text2text.tokenizer._generate_alphabet_dict">[docs]</a><span class="k">def</span> <span class="nf">_generate_alphabet_dict</span><span class="p">(</span><span class="n">iterable</span><span class="p">,</span> <span class="n">reserved_tokens</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Create set of characters that appear in any element in the iterable.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">reserved_tokens</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">reserved_tokens</span> <span class="o">=</span> <span class="n">RESERVED_TOKENS</span>
  <span class="n">alphabet</span> <span class="o">=</span> <span class="p">{</span><span class="n">c</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">iterable</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">token</span><span class="p">}</span>
  <span class="n">alphabet</span> <span class="o">|=</span> <span class="p">{</span><span class="n">c</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">reserved_tokens</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">token</span><span class="p">}</span>
  <span class="n">alphabet</span> <span class="o">|=</span> <span class="n">_ESCAPE_CHARS</span>  <span class="c1"># Add escape characters to alphabet set.</span>
  <span class="k">return</span> <span class="n">alphabet</span></div>


<div class="viewcode-block" id="_count_and_gen_subtokens"><a class="viewcode-back" href="../../../api-docs/data.text2text.html#data.text2text.tokenizer._count_and_gen_subtokens">[docs]</a><span class="k">def</span> <span class="nf">_count_and_gen_subtokens</span><span class="p">(</span>
    <span class="n">token_counts</span><span class="p">,</span> <span class="n">alphabet</span><span class="p">,</span> <span class="n">subtoken_dict</span><span class="p">,</span> <span class="n">max_subtoken_length</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Count number of times subtokens appear, and generate new subtokens.</span>

<span class="sd">  Args:</span>
<span class="sd">    token_counts: dict mapping tokens to the number of times they appear in the</span>
<span class="sd">      original files.</span>
<span class="sd">    alphabet: list of allowed characters. Used to escape the tokens, which</span>
<span class="sd">      guarantees that all tokens can be split into subtokens.</span>
<span class="sd">    subtoken_dict: dict mapping subtokens to ids.</span>
<span class="sd">    max_subtoken_length: maximum length of subtoken in subtoken_dict.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A defaultdict mapping subtokens to the number of times they appear in the</span>
<span class="sd">    tokens. The dict may contain new subtokens.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">subtoken_counts</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(</span><span class="n">token_counts</span><span class="p">):</span>
    <span class="n">token</span> <span class="o">=</span> <span class="n">_escape_token</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">alphabet</span><span class="p">)</span>
    <span class="n">subtokens</span> <span class="o">=</span> <span class="n">_split_token_to_subtokens</span><span class="p">(</span>
        <span class="n">token</span><span class="p">,</span> <span class="n">subtoken_dict</span><span class="p">,</span> <span class="n">max_subtoken_length</span><span class="p">)</span>

    <span class="c1"># Generate new subtokens by taking substrings from token.</span>
    <span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">subtoken</span> <span class="ow">in</span> <span class="n">subtokens</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">end</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="n">start</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">new_subtoken</span> <span class="o">=</span> <span class="n">token</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span>
        <span class="n">subtoken_counts</span><span class="p">[</span><span class="n">new_subtoken</span><span class="p">]</span> <span class="o">+=</span> <span class="n">count</span>
      <span class="n">start</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">subtoken</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">subtoken_counts</span></div>


<div class="viewcode-block" id="_filter_and_bucket_subtokens"><a class="viewcode-back" href="../../../api-docs/data.text2text.html#data.text2text.tokenizer._filter_and_bucket_subtokens">[docs]</a><span class="k">def</span> <span class="nf">_filter_and_bucket_subtokens</span><span class="p">(</span><span class="n">subtoken_counts</span><span class="p">,</span> <span class="n">min_count</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Return a bucketed list of subtokens that are filtered by count.</span>

<span class="sd">  Args:</span>
<span class="sd">    subtoken_counts: defaultdict mapping subtokens to their counts</span>
<span class="sd">    min_count: int count used to filter subtokens</span>

<span class="sd">  Returns:</span>
<span class="sd">    List of subtoken sets, where subtokens in set i have the same length=i.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># Create list of buckets, where subtokens in bucket i have length i.</span>
  <span class="n">subtoken_buckets</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">subtoken</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(</span><span class="n">subtoken_counts</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">count</span> <span class="o">&lt;</span> <span class="n">min_count</span><span class="p">:</span>  <span class="c1"># Filter out subtokens that don&#39;t appear enough</span>
      <span class="k">continue</span>
    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">subtoken_buckets</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">subtoken</span><span class="p">):</span>
      <span class="n">subtoken_buckets</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">set</span><span class="p">())</span>
    <span class="n">subtoken_buckets</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">subtoken</span><span class="p">)]</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">subtoken</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">subtoken_buckets</span></div>


<div class="viewcode-block" id="_gen_new_subtoken_list"><a class="viewcode-back" href="../../../api-docs/data.text2text.html#data.text2text.tokenizer._gen_new_subtoken_list">[docs]</a><span class="k">def</span> <span class="nf">_gen_new_subtoken_list</span><span class="p">(</span>
    <span class="n">subtoken_counts</span><span class="p">,</span> <span class="n">min_count</span><span class="p">,</span> <span class="n">alphabet</span><span class="p">,</span> <span class="n">reserved_tokens</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Generate candidate subtokens ordered by count, and new max subtoken length.</span>

<span class="sd">  Add subtokens to the candiate list in order of length (longest subtokens</span>
<span class="sd">  first). When a subtoken is added, the counts of each of its prefixes are</span>
<span class="sd">  decreased. Prefixes that don&#39;t appear much outside the subtoken are not added</span>
<span class="sd">  to the candidate list.</span>

<span class="sd">  For example:</span>
<span class="sd">    subtoken being added to candidate list: &#39;translate&#39;</span>
<span class="sd">    subtoken_counts: {&#39;translate&#39;:10, &#39;t&#39;:40, &#39;tr&#39;:16, &#39;tra&#39;:12, ...}</span>
<span class="sd">    min_count: 5</span>

<span class="sd">  When &#39;translate&#39; is added, subtoken_counts is updated to:</span>
<span class="sd">    {&#39;translate&#39;:0, &#39;t&#39;:30, &#39;tr&#39;:6, &#39;tra&#39;: 2, ...}</span>

<span class="sd">  The subtoken &#39;tra&#39; will not be added to the candidate list, because it appears</span>
<span class="sd">  twice (less than min_count) outside of &#39;translate&#39;.</span>

<span class="sd">  Args:</span>
<span class="sd">    subtoken_counts: defaultdict mapping str subtokens to int counts</span>
<span class="sd">    min_count: int minumum count requirement for subtokens</span>
<span class="sd">    alphabet: set of characters. Each character is added to the subtoken list to</span>
<span class="sd">      guarantee that all tokens can be encoded.</span>
<span class="sd">    reserved_tokens: list of tokens that will be added to the beginning of the</span>
<span class="sd">      returned subtoken list.</span>

<span class="sd">  Returns:</span>
<span class="sd">    List of candidate subtokens in decreasing count order, and maximum subtoken</span>
<span class="sd">    length</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">reserved_tokens</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">reserved_tokens</span> <span class="o">=</span> <span class="n">RESERVED_TOKENS</span>

  <span class="c1"># Create a list of (count, subtoken) for each candidate subtoken.</span>
  <span class="n">subtoken_candidates</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="c1"># Use bucketted list to iterate through subtokens in order of length.</span>
  <span class="c1"># subtoken_buckets[i] = set(subtokens), where each subtoken has length i.</span>
  <span class="n">subtoken_buckets</span> <span class="o">=</span> <span class="n">_filter_and_bucket_subtokens</span><span class="p">(</span><span class="n">subtoken_counts</span><span class="p">,</span> <span class="n">min_count</span><span class="p">)</span>
  <span class="n">max_subtoken_length</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">subtoken_buckets</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>

  <span class="c1"># Go through the list in reverse order to consider longer subtokens first.</span>
  <span class="k">for</span> <span class="n">subtoken_len</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="n">max_subtoken_length</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">subtoken</span> <span class="ow">in</span> <span class="n">subtoken_buckets</span><span class="p">[</span><span class="n">subtoken_len</span><span class="p">]:</span>
      <span class="n">count</span> <span class="o">=</span> <span class="n">subtoken_counts</span><span class="p">[</span><span class="n">subtoken</span><span class="p">]</span>

      <span class="c1"># Possible if this subtoken is a prefix of another token.</span>
      <span class="k">if</span> <span class="n">count</span> <span class="o">&lt;</span> <span class="n">min_count</span><span class="p">:</span>
        <span class="k">continue</span>

      <span class="c1"># Ignore alphabet/reserved tokens, which will be added manually later.</span>
      <span class="k">if</span> <span class="n">subtoken</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">alphabet</span> <span class="ow">and</span> <span class="n">subtoken</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">reserved_tokens</span><span class="p">:</span>
        <span class="n">subtoken_candidates</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">count</span><span class="p">,</span> <span class="n">subtoken</span><span class="p">))</span>

      <span class="c1"># Decrement count of the subtoken&#39;s prefixes (if a longer subtoken is</span>
      <span class="c1"># added, its prefixes lose priority to be added).</span>
      <span class="k">for</span> <span class="n">end</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">subtoken_len</span><span class="p">):</span>
        <span class="n">subtoken_counts</span><span class="p">[</span><span class="n">subtoken</span><span class="p">[:</span><span class="n">end</span><span class="p">]]</span> <span class="o">-=</span> <span class="n">count</span>

  <span class="c1"># Add alphabet subtokens (guarantees that all strings are encodable).</span>
  <span class="n">subtoken_candidates</span><span class="o">.</span><span class="n">extend</span><span class="p">((</span><span class="n">subtoken_counts</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">a</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">alphabet</span><span class="p">)</span>

  <span class="c1"># Order subtoken candidates by decreasing count.</span>
  <span class="n">subtoken_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">subtoken_candidates</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)]</span>

  <span class="c1"># Add reserved tokens to beginning of the list.</span>
  <span class="n">subtoken_list</span> <span class="o">=</span> <span class="n">reserved_tokens</span> <span class="o">+</span> <span class="n">subtoken_list</span>
  <span class="k">return</span> <span class="n">subtoken_list</span><span class="p">,</span> <span class="n">max_subtoken_length</span></div>


<div class="viewcode-block" id="_generate_subtokens"><a class="viewcode-back" href="../../../api-docs/data.text2text.html#data.text2text.tokenizer._generate_subtokens">[docs]</a><span class="k">def</span> <span class="nf">_generate_subtokens</span><span class="p">(</span>
    <span class="n">token_counts</span><span class="p">,</span> <span class="n">alphabet</span><span class="p">,</span> <span class="n">min_count</span><span class="p">,</span> <span class="n">num_iterations</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">reserved_tokens</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Create a list of subtokens in decreasing order of frequency.</span>

<span class="sd">  Args:</span>
<span class="sd">    token_counts: dict mapping str tokens -&gt; int count</span>
<span class="sd">    alphabet: set of characters</span>
<span class="sd">    min_count: int minimum number of times a subtoken must appear before it is</span>
<span class="sd">      added to the vocabulary.</span>
<span class="sd">    num_iterations: int number of iterations to generate new tokens.</span>
<span class="sd">    reserved_tokens: list of tokens that will be added to the beginning to the</span>
<span class="sd">      returned subtoken list.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Sorted list of subtokens (most frequent first)</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">reserved_tokens</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">reserved_tokens</span> <span class="o">=</span> <span class="n">RESERVED_TOKENS</span>

  <span class="c1"># Use alphabet set to create initial list of subtokens</span>
  <span class="n">subtoken_list</span> <span class="o">=</span> <span class="n">reserved_tokens</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">alphabet</span><span class="p">)</span>
  <span class="n">max_subtoken_length</span> <span class="o">=</span> <span class="mi">1</span>

  <span class="c1"># On each iteration, segment all words using the subtokens defined in</span>
  <span class="c1"># subtoken_dict, count how often the resulting subtokens appear, and update</span>
  <span class="c1"># the dictionary with subtokens w/ high enough counts.</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">):</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">Generating subtokens: iteration </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">i</span><span class="p">)</span>
    <span class="c1"># Generate new subtoken-&gt;id dictionary using the new subtoken list.</span>
    <span class="n">subtoken_dict</span> <span class="o">=</span> <span class="n">_list_to_index_dict</span><span class="p">(</span><span class="n">subtoken_list</span><span class="p">)</span>

    <span class="c1"># Create dict mapping subtoken-&gt;count, with additional subtokens created</span>
    <span class="c1"># from substrings taken from the tokens.</span>
    <span class="n">subtoken_counts</span> <span class="o">=</span> <span class="n">_count_and_gen_subtokens</span><span class="p">(</span>
        <span class="n">token_counts</span><span class="p">,</span> <span class="n">alphabet</span><span class="p">,</span> <span class="n">subtoken_dict</span><span class="p">,</span> <span class="n">max_subtoken_length</span><span class="p">)</span>

    <span class="c1"># Generate new list of subtokens sorted by subtoken count.</span>
    <span class="n">subtoken_list</span><span class="p">,</span> <span class="n">max_subtoken_length</span> <span class="o">=</span> <span class="n">_gen_new_subtoken_list</span><span class="p">(</span>
        <span class="n">subtoken_counts</span><span class="p">,</span> <span class="n">min_count</span><span class="p">,</span> <span class="n">alphabet</span><span class="p">,</span> <span class="n">reserved_tokens</span><span class="p">)</span>

    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">Vocab size: </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">subtoken_list</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">subtoken_list</span></div>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, NVIDIA.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../../',
            VERSION:'0.2',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>
  

  <script type="text/javascript">
      jQuery(function () {
          
          SphinxRtdTheme.Navigation.enableSticky();
          
      });
  </script>  
  <style>
    /* Sidebar header (and topbar for mobile) */
    .wy-side-nav-search, .wy-nav-top {
      background: #64d81c;
    }
    .wy-side-nav-search > div.version {
      color: #ffffff;
    }
    .wy-side-nav-search > img {
      max-width: 150px;
    }
    .wy-side-nav-search > a {
      font-size: 23px;
    }
  </style>


</body>
</html>