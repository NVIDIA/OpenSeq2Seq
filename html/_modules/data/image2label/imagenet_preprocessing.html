

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>data.image2label.imagenet_preprocessing &mdash; OpenSeq2Seq 0.2 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../../../_static/favicon.ico"/>
  
  
  

  

  
  
    

  

  
    <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/theme_override.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/theme_override.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../../index.html" class="icon icon-home"> OpenSeq2Seq
          

          
            
            <img src="../../../_static/logo.png" class="logo" alt="Logo"/>
          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../index.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../installation.html">Installation instructions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../machine-translation.html">Machine Translation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../speech-recognition.html">Speech Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../speech-synthesis.html">Speech Synthesis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distr-training.html">Distributed training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mixed-precision.html">Mixed precision training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../in-depth-tutorials.html">In-depth tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../interactive-infer-demos.html">Interactive Infer Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api-docs/modules.html">API documentation</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">OpenSeq2Seq</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
      <li>data.image2label.imagenet_preprocessing</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for data.image2label.imagenet_preprocessing</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2016 The TensorFlow Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ==============================================================================</span>
<span class="sd">&quot;&quot;&quot;Provides utilities to preprocess images.</span>
<span class="sd">Training images are sampled using the provided bounding boxes, and subsequently</span>
<span class="sd">cropped to the sampled bounding box. Images are additionally flipped randomly,</span>
<span class="sd">then resized to the target output size (without aspect-ratio preservation).</span>
<span class="sd">Images used during evaluation are resized (with aspect-ratio preservation) and</span>
<span class="sd">centrally cropped.</span>
<span class="sd">All images undergo mean color subtraction.</span>
<span class="sd">Note that these steps are colloquially referred to as &quot;ResNet preprocessing,&quot;</span>
<span class="sd">and they differ from &quot;VGG preprocessing,&quot; which does not use bounding boxes</span>
<span class="sd">and instead does an aspect-preserving resize followed by random crop during</span>
<span class="sd">training. (These both differ from &quot;Inception preprocessing,&quot; which introduces</span>
<span class="sd">color distortion steps.)</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="n">_R_MEAN</span> <span class="o">=</span> <span class="mf">123.68</span>
<span class="n">_G_MEAN</span> <span class="o">=</span> <span class="mf">116.78</span>
<span class="n">_B_MEAN</span> <span class="o">=</span> <span class="mf">103.94</span>
<span class="n">_CHANNEL_MEANS</span> <span class="o">=</span> <span class="p">[</span><span class="n">_R_MEAN</span><span class="p">,</span> <span class="n">_G_MEAN</span><span class="p">,</span> <span class="n">_B_MEAN</span><span class="p">]</span>

<span class="c1"># The lower bound for the smallest side of the image for aspect-preserving</span>
<span class="c1"># resizing. For example, if an image is 500 x 1000, it will be resized to</span>
<span class="c1"># _RESIZE_MIN x (_RESIZE_MIN * 2).</span>
<span class="n">_RESIZE_MIN</span> <span class="o">=</span> <span class="mi">256</span>


<div class="viewcode-block" id="_decode_crop_and_flip"><a class="viewcode-back" href="../../../api-docs/data.image2label.html#data.image2label.imagenet_preprocessing._decode_crop_and_flip">[docs]</a><span class="k">def</span> <span class="nf">_decode_crop_and_flip</span><span class="p">(</span><span class="n">image_buffer</span><span class="p">,</span> <span class="n">bbox</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Crops the given image to a random part of the image, and randomly flips.</span>
<span class="sd">  We use the fused decode_and_crop op, which performs better than the two ops</span>
<span class="sd">  used separately in series, but note that this requires that the image be</span>
<span class="sd">  passed in as an un-decoded string Tensor.</span>

<span class="sd">  Args:</span>
<span class="sd">    image_buffer: scalar string Tensor representing the raw JPEG image buffer.</span>
<span class="sd">    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]</span>
<span class="sd">      where each coordinate is [0, 1) and the coordinates are arranged as</span>
<span class="sd">      [ymin, xmin, ymax, xmax].</span>
<span class="sd">    num_channels: Integer depth of the image buffer for decoding.</span>

<span class="sd">  Returns:</span>
<span class="sd">    3-D tensor with cropped image.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># A large fraction of image datasets contain a human-annotated bounding box</span>
  <span class="c1"># delineating the region of the image containing the object of interest.  We</span>
  <span class="c1"># choose to create a new bounding box for the object which is a randomly</span>
  <span class="c1"># distorted version of the human-annotated bounding box that obeys an</span>
  <span class="c1"># allowed range of aspect ratios, sizes and overlap with the human-annotated</span>
  <span class="c1"># bounding box. If no box is supplied, then we assume the bounding box is</span>
  <span class="c1"># the entire image.</span>
  <span class="n">sample_distorted_bounding_box</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">sample_distorted_bounding_box</span><span class="p">(</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">extract_jpeg_shape</span><span class="p">(</span><span class="n">image_buffer</span><span class="p">),</span>
      <span class="n">bounding_boxes</span><span class="o">=</span><span class="n">bbox</span><span class="p">,</span>
      <span class="n">min_object_covered</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
      <span class="n">aspect_ratio_range</span><span class="o">=</span><span class="p">[</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">1.33</span><span class="p">],</span>
      <span class="n">area_range</span><span class="o">=</span><span class="p">[</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span>
      <span class="n">max_attempts</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
      <span class="n">use_image_if_no_bounding_boxes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="n">bbox_begin</span><span class="p">,</span> <span class="n">bbox_size</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sample_distorted_bounding_box</span>

  <span class="c1"># Reassemble the bounding box in the format the crop op requires.</span>
  <span class="n">offset_y</span><span class="p">,</span> <span class="n">offset_x</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">unstack</span><span class="p">(</span><span class="n">bbox_begin</span><span class="p">)</span>
  <span class="n">target_height</span><span class="p">,</span> <span class="n">target_width</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">unstack</span><span class="p">(</span><span class="n">bbox_size</span><span class="p">)</span>
  <span class="n">crop_window</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">offset_y</span><span class="p">,</span> <span class="n">offset_x</span><span class="p">,</span> <span class="n">target_height</span><span class="p">,</span> <span class="n">target_width</span><span class="p">])</span>

  <span class="c1"># Use the fused decode and crop op here, which is faster than each in series.</span>
  <span class="n">cropped</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">decode_and_crop_jpeg</span><span class="p">(</span>
      <span class="n">image_buffer</span><span class="p">,</span> <span class="n">crop_window</span><span class="p">,</span> <span class="n">channels</span><span class="o">=</span><span class="n">num_channels</span><span class="p">)</span>

  <span class="c1"># Flip to add a little more random distortion in.</span>
  <span class="n">cropped</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">random_flip_left_right</span><span class="p">(</span><span class="n">cropped</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">cropped</span></div>


<div class="viewcode-block" id="_central_crop"><a class="viewcode-back" href="../../../api-docs/data.image2label.html#data.image2label.imagenet_preprocessing._central_crop">[docs]</a><span class="k">def</span> <span class="nf">_central_crop</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">crop_height</span><span class="p">,</span> <span class="n">crop_width</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Performs central crops of the given image list.</span>

<span class="sd">  Args:</span>
<span class="sd">    image: a 3-D image tensor</span>
<span class="sd">    crop_height: the height of the image following the crop.</span>
<span class="sd">    crop_width: the width of the image following the crop.</span>

<span class="sd">  Returns:</span>
<span class="sd">    3-D tensor with cropped image.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
  <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

  <span class="n">amount_to_be_cropped_h</span> <span class="o">=</span> <span class="p">(</span><span class="n">height</span> <span class="o">-</span> <span class="n">crop_height</span><span class="p">)</span>
  <span class="n">crop_top</span> <span class="o">=</span> <span class="n">amount_to_be_cropped_h</span> <span class="o">//</span> <span class="mi">2</span>
  <span class="n">amount_to_be_cropped_w</span> <span class="o">=</span> <span class="p">(</span><span class="n">width</span> <span class="o">-</span> <span class="n">crop_width</span><span class="p">)</span>
  <span class="n">crop_left</span> <span class="o">=</span> <span class="n">amount_to_be_cropped_w</span> <span class="o">//</span> <span class="mi">2</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span>
      <span class="n">image</span><span class="p">,</span> <span class="p">[</span><span class="n">crop_top</span><span class="p">,</span> <span class="n">crop_left</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="n">crop_height</span><span class="p">,</span> <span class="n">crop_width</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span></div>


<div class="viewcode-block" id="_mean_image_subtraction_and_normalization"><a class="viewcode-back" href="../../../api-docs/data.image2label.html#data.image2label.imagenet_preprocessing._mean_image_subtraction_and_normalization">[docs]</a><span class="k">def</span> <span class="nf">_mean_image_subtraction_and_normalization</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">means</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Subtracts the given means from each image channel and divides by 127.5.</span>

<span class="sd">  For example:</span>
<span class="sd">    means = [123.68, 116.779, 103.939]</span>
<span class="sd">    image = _mean_image_subtraction_and_normalization(image, means)</span>

<span class="sd">  Note that the rank of `image` must be known.</span>

<span class="sd">  Args:</span>
<span class="sd">    image: a tensor of size [height, width, C].</span>
<span class="sd">    means: a C-vector of values to subtract from each channel.</span>
<span class="sd">    num_channels: number of color channels in the image that will be distorted.</span>

<span class="sd">  Returns:</span>
<span class="sd">    the centered image and normalized image.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If the rank of `image` is unknown, if `image` has a rank other</span>
<span class="sd">      than three or if the number of channels in `image` doesn&#39;t match the</span>
<span class="sd">      number of values in `means`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">image</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">ndims</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Input must be of size [height, width, C&gt;0]&#39;</span><span class="p">)</span>

  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">means</span><span class="p">)</span> <span class="o">!=</span> <span class="n">num_channels</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;len(means) must match the number of channels&#39;</span><span class="p">)</span>

  <span class="c1"># We have a 1-D tensor of means; convert to 3-D.</span>
  <span class="n">means</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">means</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>

  <span class="k">return</span> <span class="p">(</span><span class="n">image</span> <span class="o">-</span> <span class="n">means</span><span class="p">)</span> <span class="o">/</span> <span class="mf">127.5</span></div>


<div class="viewcode-block" id="_smallest_size_at_least"><a class="viewcode-back" href="../../../api-docs/data.image2label.html#data.image2label.imagenet_preprocessing._smallest_size_at_least">[docs]</a><span class="k">def</span> <span class="nf">_smallest_size_at_least</span><span class="p">(</span><span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">resize_min</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes new shape with the smallest side equal to `smallest_side`.</span>
<span class="sd">  Computes new shape with the smallest side equal to `smallest_side` while</span>
<span class="sd">  preserving the original aspect ratio.</span>

<span class="sd">  Args:</span>
<span class="sd">    height: an int32 scalar tensor indicating the current height.</span>
<span class="sd">    width: an int32 scalar tensor indicating the current width.</span>
<span class="sd">    resize_min: A python integer or scalar `Tensor` indicating the size of</span>
<span class="sd">      the smallest side after resize.</span>

<span class="sd">  Returns:</span>
<span class="sd">    new_height: an int32 scalar tensor indicating the new height.</span>
<span class="sd">    new_width: an int32 scalar tensor indicating the new width.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">resize_min</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">resize_min</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

  <span class="c1"># Convert to floats to make subsequent calculations go smoothly.</span>
  <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">height</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">width</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

  <span class="n">smaller_dim</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>
  <span class="n">scale_ratio</span> <span class="o">=</span> <span class="n">resize_min</span> <span class="o">/</span> <span class="n">smaller_dim</span>

  <span class="c1"># Convert back to ints to make heights and widths that TF ops will accept.</span>
  <span class="n">new_height</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">height</span> <span class="o">*</span> <span class="n">scale_ratio</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
  <span class="n">new_width</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">width</span> <span class="o">*</span> <span class="n">scale_ratio</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">new_height</span><span class="p">,</span> <span class="n">new_width</span></div>


<div class="viewcode-block" id="_aspect_preserving_resize"><a class="viewcode-back" href="../../../api-docs/data.image2label.html#data.image2label.imagenet_preprocessing._aspect_preserving_resize">[docs]</a><span class="k">def</span> <span class="nf">_aspect_preserving_resize</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">resize_min</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Resize images preserving the original aspect ratio.</span>

<span class="sd">  Args:</span>
<span class="sd">    image: A 3-D image `Tensor`.</span>
<span class="sd">    resize_min: A python integer or scalar `Tensor` indicating the size of</span>
<span class="sd">      the smallest side after resize.</span>

<span class="sd">  Returns:</span>
<span class="sd">    resized_image: A 3-D tensor containing the resized image.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
  <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

  <span class="n">new_height</span><span class="p">,</span> <span class="n">new_width</span> <span class="o">=</span> <span class="n">_smallest_size_at_least</span><span class="p">(</span><span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">resize_min</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">_resize_image</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">new_height</span><span class="p">,</span> <span class="n">new_width</span><span class="p">)</span></div>


<div class="viewcode-block" id="_resize_image"><a class="viewcode-back" href="../../../api-docs/data.image2label.html#data.image2label.imagenet_preprocessing._resize_image">[docs]</a><span class="k">def</span> <span class="nf">_resize_image</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Simple wrapper around tf.resize_images.</span>
<span class="sd">  This is primarily to make sure we use the same `ResizeMethod` and other</span>
<span class="sd">  details each time.</span>

<span class="sd">  Args:</span>
<span class="sd">    image: A 3-D image `Tensor`.</span>
<span class="sd">    height: The target height for the resized image.</span>
<span class="sd">    width: The target width for the resized image.</span>

<span class="sd">  Returns:</span>
<span class="sd">    resized_image: A 3-D tensor containing the resized image. The first two</span>
<span class="sd">      dimensions have the shape [height, width].</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">resize_images</span><span class="p">(</span>
      <span class="n">image</span><span class="p">,</span> <span class="p">[</span><span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">],</span> <span class="n">method</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">ResizeMethod</span><span class="o">.</span><span class="n">BILINEAR</span><span class="p">,</span>
      <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></div>


<div class="viewcode-block" id="preprocess_image"><a class="viewcode-back" href="../../../api-docs/data.image2label.html#data.image2label.imagenet_preprocessing.preprocess_image">[docs]</a><span class="k">def</span> <span class="nf">preprocess_image</span><span class="p">(</span><span class="n">image_buffer</span><span class="p">,</span> <span class="n">bbox</span><span class="p">,</span> <span class="n">output_height</span><span class="p">,</span> <span class="n">output_width</span><span class="p">,</span>
                     <span class="n">num_channels</span><span class="p">,</span> <span class="n">is_training</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Preprocesses the given image.</span>
<span class="sd">  Preprocessing includes decoding, cropping, and resizing for both training</span>
<span class="sd">  and eval images. Training preprocessing, however, introduces some random</span>
<span class="sd">  distortion of the image to improve accuracy.</span>

<span class="sd">  Args:</span>
<span class="sd">    image_buffer: scalar string Tensor representing the raw JPEG image buffer.</span>
<span class="sd">    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]</span>
<span class="sd">      where each coordinate is [0, 1) and the coordinates are arranged as</span>
<span class="sd">      [ymin, xmin, ymax, xmax].</span>
<span class="sd">    output_height: The height of the image after preprocessing.</span>
<span class="sd">    output_width: The width of the image after preprocessing.</span>
<span class="sd">    num_channels: Integer depth of the image buffer for decoding.</span>
<span class="sd">    is_training: `True` if we&#39;re preprocessing the image for training and</span>
<span class="sd">      `False` otherwise.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A preprocessed image.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">is_training</span><span class="p">:</span>
    <span class="c1"># For training, we want to randomize some of the distortions.</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">_decode_crop_and_flip</span><span class="p">(</span><span class="n">image_buffer</span><span class="p">,</span> <span class="n">bbox</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">)</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">_resize_image</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">output_height</span><span class="p">,</span> <span class="n">output_width</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="c1"># For validation, we want to decode, resize, then just crop the middle.</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">decode_jpeg</span><span class="p">(</span><span class="n">image_buffer</span><span class="p">,</span> <span class="n">channels</span><span class="o">=</span><span class="n">num_channels</span><span class="p">)</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">_aspect_preserving_resize</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">_RESIZE_MIN</span><span class="p">)</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">_central_crop</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">output_height</span><span class="p">,</span> <span class="n">output_width</span><span class="p">)</span>

  <span class="n">image</span><span class="o">.</span><span class="n">set_shape</span><span class="p">([</span><span class="n">output_height</span><span class="p">,</span> <span class="n">output_width</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">])</span>

  <span class="k">return</span> <span class="n">_mean_image_subtraction_and_normalization</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">_CHANNEL_MEANS</span><span class="p">,</span>
                                                   <span class="n">num_channels</span><span class="p">)</span></div>


<div class="viewcode-block" id="_parse_example_proto"><a class="viewcode-back" href="../../../api-docs/data.image2label.html#data.image2label.imagenet_preprocessing._parse_example_proto">[docs]</a><span class="k">def</span> <span class="nf">_parse_example_proto</span><span class="p">(</span><span class="n">example_serialized</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Parses an Example proto containing a training example of an image.</span>
<span class="sd">  The output of the build_image_data.py image preprocessing script is a dataset</span>
<span class="sd">  containing serialized Example protocol buffers. Each Example proto contains</span>
<span class="sd">  the following fields (values are included as examples):</span>
<span class="sd">    image/height: 462</span>
<span class="sd">    image/width: 581</span>
<span class="sd">    image/colorspace: &#39;RGB&#39;</span>
<span class="sd">    image/channels: 3</span>
<span class="sd">    image/class/label: 615</span>
<span class="sd">    image/class/synset: &#39;n03623198&#39;</span>
<span class="sd">    image/class/text: &#39;knee pad&#39;</span>
<span class="sd">    image/object/bbox/xmin: 0.1</span>
<span class="sd">    image/object/bbox/xmax: 0.9</span>
<span class="sd">    image/object/bbox/ymin: 0.2</span>
<span class="sd">    image/object/bbox/ymax: 0.6</span>
<span class="sd">    image/object/bbox/label: 615</span>
<span class="sd">    image/format: &#39;JPEG&#39;</span>
<span class="sd">    image/filename: &#39;ILSVRC2012_val_00041207.JPEG&#39;</span>
<span class="sd">    image/encoded: &lt;JPEG encoded string&gt;</span>

<span class="sd">  Args:</span>
<span class="sd">    example_serialized: scalar Tensor tf.string containing a serialized</span>
<span class="sd">      Example protocol buffer.</span>

<span class="sd">  Returns:</span>
<span class="sd">    image_buffer: Tensor tf.string containing the contents of a JPEG file.</span>
<span class="sd">    label: Tensor tf.int32 containing the label.</span>
<span class="sd">    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]</span>
<span class="sd">      where each coordinate is [0, 1) and the coordinates are arranged as</span>
<span class="sd">      [ymin, xmin, ymax, xmax].</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># Dense features in Example proto.</span>
  <span class="n">feature_map</span> <span class="o">=</span> <span class="p">{</span>
      <span class="s1">&#39;image/encoded&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">FixedLenFeature</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">string</span><span class="p">,</span>
                                          <span class="n">default_value</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">),</span>
      <span class="s1">&#39;image/class/label&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">FixedLenFeature</span><span class="p">([</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span>
                                              <span class="n">default_value</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
      <span class="s1">&#39;image/class/text&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">FixedLenFeature</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">string</span><span class="p">,</span>
                                             <span class="n">default_value</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">),</span>
  <span class="p">}</span>
  <span class="n">sparse_float32</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">VarLenFeature</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="c1"># Sparse features in Example proto.</span>
  <span class="n">feature_map</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
      <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">sparse_float32</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;image/object/bbox/xmin&#39;</span><span class="p">,</span>
                                   <span class="s1">&#39;image/object/bbox/ymin&#39;</span><span class="p">,</span>
                                   <span class="s1">&#39;image/object/bbox/xmax&#39;</span><span class="p">,</span>
                                   <span class="s1">&#39;image/object/bbox/ymax&#39;</span><span class="p">]})</span>

  <span class="n">features</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">parse_single_example</span><span class="p">(</span><span class="n">example_serialized</span><span class="p">,</span> <span class="n">feature_map</span><span class="p">)</span>
  <span class="n">label</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="s1">&#39;image/class/label&#39;</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

  <span class="n">xmin</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="s1">&#39;image/object/bbox/xmin&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
  <span class="n">ymin</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="s1">&#39;image/object/bbox/ymin&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
  <span class="n">xmax</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="s1">&#39;image/object/bbox/xmax&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
  <span class="n">ymax</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="s1">&#39;image/object/bbox/ymax&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

  <span class="c1"># Note that we impose an ordering of (y, x) just to make life difficult.</span>
  <span class="n">bbox</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">ymin</span><span class="p">,</span> <span class="n">xmin</span><span class="p">,</span> <span class="n">ymax</span><span class="p">,</span> <span class="n">xmax</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>

  <span class="c1"># Force the variable number of bounding boxes into the shape</span>
  <span class="c1"># [1, num_boxes, coords].</span>
  <span class="n">bbox</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">bbox</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
  <span class="n">bbox</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">bbox</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

  <span class="k">return</span> <span class="n">features</span><span class="p">[</span><span class="s1">&#39;image/encoded&#39;</span><span class="p">],</span> <span class="n">label</span><span class="p">,</span> <span class="n">bbox</span></div>


<div class="viewcode-block" id="parse_record"><a class="viewcode-back" href="../../../api-docs/data.image2label.html#data.image2label.imagenet_preprocessing.parse_record">[docs]</a><span class="k">def</span> <span class="nf">parse_record</span><span class="p">(</span><span class="n">raw_record</span><span class="p">,</span> <span class="n">is_training</span><span class="p">,</span> <span class="n">image_size</span><span class="o">=</span><span class="mi">224</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Parses a record containing a training example of an image.</span>
<span class="sd">  The input record is parsed into a label and image, and the image is passed</span>
<span class="sd">  through preprocessing steps (cropping, flipping, and so on).</span>

<span class="sd">  Args:</span>
<span class="sd">    raw_record: scalar Tensor tf.string containing a serialized</span>
<span class="sd">        Example protocol buffer.</span>
<span class="sd">    is_training: A boolean denoting whether the input is for training.</span>
<span class="sd">    image_size (int): size that images should be resized to.</span>
<span class="sd">    num_classes (int): number of output classes.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Tuple with processed image tensor and one-hot-encoded label tensor.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">image_buffer</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">bbox</span> <span class="o">=</span> <span class="n">_parse_example_proto</span><span class="p">(</span><span class="n">raw_record</span><span class="p">)</span>

  <span class="n">image</span> <span class="o">=</span> <span class="n">preprocess_image</span><span class="p">(</span>
      <span class="n">image_buffer</span><span class="o">=</span><span class="n">image_buffer</span><span class="p">,</span>
      <span class="n">bbox</span><span class="o">=</span><span class="n">bbox</span><span class="p">,</span>
      <span class="n">output_height</span><span class="o">=</span><span class="n">image_size</span><span class="p">,</span>
      <span class="n">output_width</span><span class="o">=</span><span class="n">image_size</span><span class="p">,</span>
      <span class="n">num_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
      <span class="n">is_training</span><span class="o">=</span><span class="n">is_training</span><span class="p">)</span>

  <span class="c1"># subtracting 1 to make labels go from 0 to 999</span>
  <span class="n">label</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">label</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[]),</span> <span class="n">num_classes</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">image</span><span class="p">,</span> <span class="n">label</span></div>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, NVIDIA.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../../',
            VERSION:'0.2',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>
  

  <script type="text/javascript">
      jQuery(function () {
          
          SphinxRtdTheme.Navigation.enableSticky();
          
      });
  </script>  
  <style>
    /* Sidebar header (and topbar for mobile) */
    .wy-side-nav-search, .wy-nav-top {
      background: #64d81c;
    }
    .wy-side-nav-search > div.version {
      color: #ffffff;
    }
    .wy-side-nav-search > img {
      max-width: 150px;
    }
    .wy-side-nav-search > a {
      font-size: 23px;
    }
  </style>


</body>
</html>