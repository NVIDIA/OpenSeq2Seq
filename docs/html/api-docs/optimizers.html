

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>optimizers &mdash; OpenSeq2Seq 0.2 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  
  
  

  

  
  
    

  

  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/theme_override.css" type="text/css" />
  <link rel="stylesheet" href="../_static/theme_override.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="parts" href="parts.html" />
    <link rel="prev" title="losses" href="losses.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> OpenSeq2Seq
          

          
            
            <img src="../_static/logo.png" class="logo" alt="Logo"/>
          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation-instructions.html">Installation instructions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting-started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../models-and-recipes.html">Models and recipes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distr-training.html">Distributed training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mixed-precision.html">Mixed precision training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../in-depth-tutorials.html">In-depth tutorials</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="modules.html">API documentation</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="models.html">models</a></li>
<li class="toctree-l2"><a class="reference internal" href="data.html">data</a></li>
<li class="toctree-l2"><a class="reference internal" href="encoders.html">encoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="decoders.html">decoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="losses.html">losses</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">optimizers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id1">optimizers</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-optimizers.mp_wrapper">mp_wrapper</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-optimizers.automatic_loss_scaler">automatic_loss_scaler</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-optimizers.lr_policies">lr_policies</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="parts.html">parts</a></li>
<li class="toctree-l2"><a class="reference internal" href="utils.html">utils</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">OpenSeq2Seq</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="modules.html">API documentation</a> &raquo;</li>
        
      <li>optimizers</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/api-docs/optimizers.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-optimizers">
<span id="optimizers"></span><h1>optimizers<a class="headerlink" href="#module-optimizers" title="Permalink to this headline">¶</a></h1>
<div class="section" id="id1">
<h2>optimizers<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-optimizers.optimizers"></span><p>Optimizer ops for use in layers and tf.learn.</p>
<dl class="function">
<dt id="optimizers.optimizers._clip_gradients_by_norm">
<code class="descclassname">optimizers.optimizers.</code><code class="descname">_clip_gradients_by_norm</code><span class="sig-paren">(</span><em>grads_and_vars</em>, <em>clip_gradients</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/optimizers/optimizers.html#_clip_gradients_by_norm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#optimizers.optimizers._clip_gradients_by_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Clips gradients by global norm.</p>
</dd></dl>

<dl class="function">
<dt id="optimizers.optimizers.get_regularization_loss">
<code class="descclassname">optimizers.optimizers.</code><code class="descname">get_regularization_loss</code><span class="sig-paren">(</span><em>scope=None</em>, <em>name='total_regularization_loss'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/optimizers/optimizers.html#get_regularization_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#optimizers.optimizers.get_regularization_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Gets the total regularization loss.
:param scope: An optional scope name for filtering the losses to return.
:param name: The name of the returned tensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">A scalar regularization loss.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="optimizers.optimizers.optimize_loss">
<code class="descclassname">optimizers.optimizers.</code><code class="descname">optimize_loss</code><span class="sig-paren">(</span><em>loss</em>, <em>optimizer</em>, <em>optimizer_params</em>, <em>learning_rate_decay_fn</em>, <em>dtype=tf.float32</em>, <em>clip_gradients=None</em>, <em>summaries=None</em>, <em>larc_params=None</em>, <em>loss_scaling=1.0</em>, <em>on_horovod=False</em>, <em>iter_size=1</em>, <em>skip_update_ph=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/optimizers/optimizers.html#optimize_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#optimizers.optimizers.optimize_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Given loss and parameters for optimizer, returns a training op.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>loss</strong> – Scalar <cite>Tensor</cite>.</li>
<li><strong>optimizer</strong> – string or class of optimizer, used as trainer.
string should be name of optimizer, like ‘SGD’,
‘Adam’, ‘Adagrad’. Full list in OPTIMIZER_CLS_NAMES constant.
class should be sub-class of <cite>tf.Optimizer</cite> that implements
<cite>compute_gradients</cite> and <cite>apply_gradients</cite> functions.</li>
<li><strong>optimizer_params</strong> – parameters of the optimizer.</li>
<li><strong>dtype</strong> – model dtype (tf.float16, tf.float32 or “mixed”).</li>
<li><strong>learning_rate_decay_fn</strong> – function, takes <cite>global_step</cite>
<cite>Tensor`s, returns `Tensor</cite>.
Can be used to implement any learning rate decay
functions.
For example: <cite>tf.train.exponential_decay</cite>.
Ignored if <cite>learning_rate</cite> is not supplied.</li>
<li><strong>clip_gradients</strong> – float, max gradient norm to clip to.</li>
<li><strong>summaries</strong> – List of internal quantities to visualize on tensorboard. If not
set only the loss and the learning rate will be reported. The
complete list is in OPTIMIZER_SUMMARIES.</li>
<li><strong>larc_params</strong> – If not None, LARC re-scaling will
be applied with corresponding parameters.</li>
<li><strong>loss_scaling</strong> – could be float or string. If float, static loss scaling
is applied. If string, the corresponding automatic
loss scaling algorithm is used. Must be one of ‘Backoff’
of ‘LogMax’ (case insensitive). Only used when dtype=”mixed”.</li>
<li><strong>on_horovod</strong> – whether the model is run on horovod.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">training op.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="optimizers.optimizers.post_process_gradients">
<code class="descclassname">optimizers.optimizers.</code><code class="descname">post_process_gradients</code><span class="sig-paren">(</span><em>grads_and_vars</em>, <em>summaries</em>, <em>lr</em>, <em>clip_gradients</em>, <em>larc_params</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/optimizers/optimizers.html#post_process_gradients"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#optimizers.optimizers.post_process_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies post processing to gradients, i.e. clipping, LARC, summaries.</p>
</dd></dl>

<dl class="function">
<dt id="optimizers.optimizers.reduce_gradients">
<code class="descclassname">optimizers.optimizers.</code><code class="descname">reduce_gradients</code><span class="sig-paren">(</span><em>grads_and_vars</em>, <em>on_horovod</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/optimizers/optimizers.html#reduce_gradients"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#optimizers.optimizers.reduce_gradients" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-optimizers.mp_wrapper">
<span id="mp-wrapper"></span><h2>mp_wrapper<a class="headerlink" href="#module-optimizers.mp_wrapper" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="optimizers.mp_wrapper.MixedPrecisionOptimizerWrapper">
<em class="property">class </em><code class="descclassname">optimizers.mp_wrapper.</code><code class="descname">MixedPrecisionOptimizerWrapper</code><span class="sig-paren">(</span><em>optimizer</em>, <em>loss_scale=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/optimizers/mp_wrapper.html#MixedPrecisionOptimizerWrapper"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#optimizers.mp_wrapper.MixedPrecisionOptimizerWrapper" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.training.optimizer.Optimizer</span></code></p>
<dl class="method">
<dt id="optimizers.mp_wrapper.MixedPrecisionOptimizerWrapper.apply_gradients">
<code class="descname">apply_gradients</code><span class="sig-paren">(</span><em>grads_and_vars</em>, <em>global_step=None</em>, <em>name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/optimizers/mp_wrapper.html#MixedPrecisionOptimizerWrapper.apply_gradients"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#optimizers.mp_wrapper.MixedPrecisionOptimizerWrapper.apply_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply gradients to variables.</p>
<p>This is the second part of <cite>minimize()</cite>. It returns an <cite>Operation</cite> that
applies gradients.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>grads_and_vars</strong> – List of (gradient, variable) pairs as returned by
<cite>compute_gradients()</cite>.</li>
<li><strong>global_step</strong> – Optional <cite>Variable</cite> to increment by one after the
variables have been updated.</li>
<li><strong>name</strong> – Optional name for the returned operation.  Default to the
name passed to the <cite>Optimizer</cite> constructor.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">An <cite>Operation</cite> that applies the specified gradients. If <cite>global_step</cite>
was not None, that operation also increments <cite>global_step</cite>.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Raises:</th><td class="field-body"><ul class="first last simple">
<li><code class="xref py py-exc docutils literal notranslate"><span class="pre">TypeError</span></code> – If <cite>grads_and_vars</cite> is malformed.</li>
<li><code class="xref py py-exc docutils literal notranslate"><span class="pre">ValueError</span></code> – If none of the variables have gradients.</li>
<li><code class="xref py py-exc docutils literal notranslate"><span class="pre">RuntimeError</span></code> – If you should use <cite>_distributed_apply()</cite> instead.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="optimizers.mp_wrapper.MixedPrecisionOptimizerWrapper.compute_gradients">
<code class="descname">compute_gradients</code><span class="sig-paren">(</span><em>loss</em>, <em>var_list=None</em>, <em>gate_gradients=1</em>, <em>aggregation_method=None</em>, <em>colocate_gradients_with_ops=False</em>, <em>grad_loss=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/optimizers/mp_wrapper.html#MixedPrecisionOptimizerWrapper.compute_gradients"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#optimizers.mp_wrapper.MixedPrecisionOptimizerWrapper.compute_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute gradients of <cite>loss</cite> for the variables in <cite>var_list</cite>.</p>
<p>This is the first part of <cite>minimize()</cite>.  It returns a list
of (gradient, variable) pairs where “gradient” is the gradient
for “variable”.  Note that “gradient” can be a <cite>Tensor</cite>, an
<cite>IndexedSlices</cite>, or <cite>None</cite> if there is no gradient for the
given variable.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>loss</strong> – A Tensor containing the value to minimize or a callable taking
no arguments which returns the value to minimize. When eager execution
is enabled it must be a callable.</li>
<li><strong>var_list</strong> – Optional list or tuple of <cite>tf.Variable</cite> to update to minimize
<cite>loss</cite>.  Defaults to the list of variables collected in the graph
under the key <cite>GraphKeys.TRAINABLE_VARIABLES</cite>.</li>
<li><strong>gate_gradients</strong> – How to gate the computation of gradients.  Can be
<cite>GATE_NONE</cite>, <cite>GATE_OP</cite>, or <cite>GATE_GRAPH</cite>.</li>
<li><strong>aggregation_method</strong> – Specifies the method used to combine gradient terms.
Valid values are defined in the class <cite>AggregationMethod</cite>.</li>
<li><strong>colocate_gradients_with_ops</strong> – If True, try colocating gradients with
the corresponding op.</li>
<li><strong>grad_loss</strong> – Optional. A <cite>Tensor</cite> holding the gradient computed for <cite>loss</cite>.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A list of (gradient, variable) pairs. Variable is always present, but
gradient can be <cite>None</cite>.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Raises:</th><td class="field-body"><ul class="first last simple">
<li><code class="xref py py-exc docutils literal notranslate"><span class="pre">TypeError</span></code> – If <cite>var_list</cite> contains anything else than <cite>Variable</cite> objects.</li>
<li><code class="xref py py-exc docutils literal notranslate"><span class="pre">ValueError</span></code> – If some arguments are invalid.</li>
<li><code class="xref py py-exc docutils literal notranslate"><span class="pre">RuntimeError</span></code> – If called with eager execution enabled and <cite>loss</cite> is
not callable.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>&#64;compatibility(eager)
When eager execution is enabled, <cite>gate_gradients</cite>, <cite>aggregation_method</cite>,
and <cite>colocate_gradients_with_ops</cite> are ignored.
&#64;end_compatibility</p>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="optimizers.mp_wrapper.mp_regularizer_wrapper">
<code class="descclassname">optimizers.mp_wrapper.</code><code class="descname">mp_regularizer_wrapper</code><span class="sig-paren">(</span><em>regularizer</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/optimizers/mp_wrapper.html#mp_regularizer_wrapper"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#optimizers.mp_wrapper.mp_regularizer_wrapper" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-optimizers.automatic_loss_scaler">
<span id="automatic-loss-scaler"></span><h2>automatic_loss_scaler<a class="headerlink" href="#module-optimizers.automatic_loss_scaler" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="optimizers.automatic_loss_scaler.AutomaticLossScaler">
<em class="property">class </em><code class="descclassname">optimizers.automatic_loss_scaler.</code><code class="descname">AutomaticLossScaler</code><span class="sig-paren">(</span><em>algorithm='Backoff'</em>, <em>scale_min=1.0</em>, <em>scale_max=16777216.0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/optimizers/automatic_loss_scaler.html#AutomaticLossScaler"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#optimizers.automatic_loss_scaler.AutomaticLossScaler" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="attribute">
<dt id="optimizers.automatic_loss_scaler.AutomaticLossScaler.SUPPORTED_ALGOS">
<code class="descname">SUPPORTED_ALGOS</code><em class="property"> = ['backoff', 'logmax']</em><a class="headerlink" href="#optimizers.automatic_loss_scaler.AutomaticLossScaler.SUPPORTED_ALGOS" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="staticmethod">
<dt id="optimizers.automatic_loss_scaler.AutomaticLossScaler.check_grads">
<em class="property">static </em><code class="descname">check_grads</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/optimizers/automatic_loss_scaler.html#AutomaticLossScaler.check_grads"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#optimizers.automatic_loss_scaler.AutomaticLossScaler.check_grads" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="optimizers.automatic_loss_scaler.AutomaticLossScaler.loss_scale">
<code class="descname">loss_scale</code><a class="headerlink" href="#optimizers.automatic_loss_scaler.AutomaticLossScaler.loss_scale" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="optimizers.automatic_loss_scaler.AutomaticLossScaler.update_op">
<code class="descname">update_op</code><span class="sig-paren">(</span><em>has_nan</em>, <em>amax</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/optimizers/automatic_loss_scaler.html#AutomaticLossScaler.update_op"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#optimizers.automatic_loss_scaler.AutomaticLossScaler.update_op" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="optimizers.automatic_loss_scaler.BackoffScaler">
<em class="property">class </em><code class="descclassname">optimizers.automatic_loss_scaler.</code><code class="descname">BackoffScaler</code><span class="sig-paren">(</span><em>scale_min</em>, <em>scale_max</em>, <em>step_factor</em>, <em>step_window</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/optimizers/automatic_loss_scaler.html#BackoffScaler"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#optimizers.automatic_loss_scaler.BackoffScaler" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="attribute">
<dt id="optimizers.automatic_loss_scaler.BackoffScaler.loss_scale">
<code class="descname">loss_scale</code><a class="headerlink" href="#optimizers.automatic_loss_scaler.BackoffScaler.loss_scale" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="optimizers.automatic_loss_scaler.BackoffScaler.update_op">
<code class="descname">update_op</code><span class="sig-paren">(</span><em>has_nan</em>, <em>amax</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/optimizers/automatic_loss_scaler.html#BackoffScaler.update_op"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#optimizers.automatic_loss_scaler.BackoffScaler.update_op" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="optimizers.automatic_loss_scaler.LogMaxScaler">
<em class="property">class </em><code class="descclassname">optimizers.automatic_loss_scaler.</code><code class="descname">LogMaxScaler</code><span class="sig-paren">(</span><em>scale_min</em>, <em>scale_max</em>, <em>log_max</em>, <em>beta1</em>, <em>beta2</em>, <em>overflow_std_dev</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/optimizers/automatic_loss_scaler.html#LogMaxScaler"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#optimizers.automatic_loss_scaler.LogMaxScaler" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="attribute">
<dt id="optimizers.automatic_loss_scaler.LogMaxScaler.loss_scale">
<code class="descname">loss_scale</code><a class="headerlink" href="#optimizers.automatic_loss_scaler.LogMaxScaler.loss_scale" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="optimizers.automatic_loss_scaler.LogMaxScaler.update_op">
<code class="descname">update_op</code><span class="sig-paren">(</span><em>has_nan</em>, <em>amax</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/optimizers/automatic_loss_scaler.html#LogMaxScaler.update_op"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#optimizers.automatic_loss_scaler.LogMaxScaler.update_op" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-optimizers.lr_policies">
<span id="lr-policies"></span><h2>lr_policies<a class="headerlink" href="#module-optimizers.lr_policies" title="Permalink to this headline">¶</a></h2>
<p>Module containing various learning rate policies. Learning rate policy can
be any function that takes arbitrary arguments from the config (with additional
<code class="docutils literal notranslate"><span class="pre">global_step</span></code> variable provided automatically) and returns learning rate
value for the current step.</p>
<dl class="function">
<dt id="optimizers.lr_policies.exp_decay">
<code class="descclassname">optimizers.lr_policies.</code><code class="descname">exp_decay</code><span class="sig-paren">(</span><em>global_step</em>, <em>learning_rate</em>, <em>decay_steps</em>, <em>decay_rate</em>, <em>use_staircase_decay</em>, <em>begin_decay_at=0</em>, <em>min_lr=0.0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/optimizers/lr_policies.html#exp_decay"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#optimizers.lr_policies.exp_decay" title="Permalink to this definition">¶</a></dt>
<dd><p>Exponential decay learning rate policy.
This function is equivalent to <code class="docutils literal notranslate"><span class="pre">tensorflow.train.exponential_decay</span></code> with
some additional functionality. Namely, it adds <code class="docutils literal notranslate"><span class="pre">begin_decay_at</span></code> parameter
and <code class="docutils literal notranslate"><span class="pre">min_lr</span></code> parameter which are the first step to start decaying learning
rate and minimal value of the learning rate correspondingly.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>global_step</strong> – global step TensorFlow tensor.</li>
<li><strong>learning_rate</strong> (<em>float</em>) – initial learning rate to use.</li>
<li><strong>decay_steps</strong> (<em>int</em>) – number of steps to apply decay for.</li>
<li><strong>decay_rate</strong> (<em>float</em>) – the rate of the decay.</li>
<li><strong>use_staircase_decay</strong> (<em>bool</em>) – whether to use staircase decay.</li>
<li><strong>begin_decay_at</strong> (<em>int</em>) – the first step to start decaying learning rate.</li>
<li><strong>min_lr</strong> (<em>float</em>) – minimal value of the learning rate.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">learning rate at step <code class="docutils literal notranslate"><span class="pre">global_step</span></code>.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="optimizers.lr_policies.fixed_lr">
<code class="descclassname">optimizers.lr_policies.</code><code class="descname">fixed_lr</code><span class="sig-paren">(</span><em>global_step</em>, <em>learning_rate</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/optimizers/lr_policies.html#fixed_lr"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#optimizers.lr_policies.fixed_lr" title="Permalink to this definition">¶</a></dt>
<dd><p>Fixed learning rate policy.
This function always returns <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code>, ignoring <code class="docutils literal notranslate"><span class="pre">global_step</span></code>
value.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>global_step</strong> – global step TensorFlow tensor (ignored for this policy).</li>
<li><strong>learning_rate</strong> (<em>float</em>) – fixed learning rate to use.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">learning rate at step <code class="docutils literal notranslate"><span class="pre">global_step</span></code>.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="optimizers.lr_policies.piecewise_constant">
<code class="descclassname">optimizers.lr_policies.</code><code class="descname">piecewise_constant</code><span class="sig-paren">(</span><em>global_step</em>, <em>learning_rate</em>, <em>boundaries</em>, <em>decay_rates</em>, <em>steps_per_epoch=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/optimizers/lr_policies.html#piecewise_constant"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#optimizers.lr_policies.piecewise_constant" title="Permalink to this definition">¶</a></dt>
<dd><p>Piecewise constant learning rate decay.
When defined in the config, only <code class="docutils literal notranslate"><span class="pre">boundaries</span></code> and <code class="docutils literal notranslate"><span class="pre">decay_rates</span></code> need to
be provided (other parameters are automatically populated by
<a class="reference internal" href="models.html#models.model.Model" title="models.model.Model"><code class="xref py py-class docutils literal notranslate"><span class="pre">Model</span></code></a> class). <code class="docutils literal notranslate"><span class="pre">boundaries</span></code> are treated as
epochs if <code class="docutils literal notranslate"><span class="pre">num_epochs</span></code> is provided in the config, otherwise treated as
steps.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>global_step</strong> – global step TensorFlow tensor.</li>
<li><strong>learning_rate</strong> (<em>float</em>) – initial learning rate to use.</li>
<li><strong>boundaries</strong> (<em>list</em>) – could be either defined in steps
(if <code class="docutils literal notranslate"><span class="pre">batches_per_epoch=None</span></code>) or in epochs if <code class="docutils literal notranslate"><span class="pre">batches_per_epoch</span></code>
parameter is defined.</li>
<li><strong>decay_rates</strong> – multiplier of the initial learning rate for each boundary.</li>
<li><strong>steps_per_epoch</strong> – number of batches in one training epoch. If provided,
boundaries are treated as epochs, otherwise as steps.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">learning rate at step <code class="docutils literal notranslate"><span class="pre">global_step</span></code>.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="optimizers.lr_policies.poly_decay">
<code class="descclassname">optimizers.lr_policies.</code><code class="descname">poly_decay</code><span class="sig-paren">(</span><em>global_step</em>, <em>learning_rate</em>, <em>decay_steps</em>, <em>power=1.0</em>, <em>begin_decay_at=0</em>, <em>min_lr=0.0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/optimizers/lr_policies.html#poly_decay"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#optimizers.lr_policies.poly_decay" title="Permalink to this definition">¶</a></dt>
<dd><p>Polynomial decay learning rate policy.
This function is equivalent to <code class="docutils literal notranslate"><span class="pre">tensorflow.train.polynomial_decay</span></code> with
some additional functionality. Namely, it adds <code class="docutils literal notranslate"><span class="pre">begin_decay_at</span></code> parameter
which is the first step to start decaying learning rate.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>global_step</strong> – global step TensorFlow tensor.</li>
<li><strong>learning_rate</strong> (<em>float</em>) – initial learning rate to use.</li>
<li><strong>decay_steps</strong> (<em>int</em>) – number of steps to apply decay for.</li>
<li><strong>power</strong> (<em>float</em>) – power for polynomial decay.</li>
<li><strong>begin_decay_at</strong> (<em>int</em>) – the first step to start decaying learning rate.</li>
<li><strong>min_lr</strong> (<em>float</em>) – minimal value of the learning rate
(same as <code class="docutils literal notranslate"><span class="pre">end_learning_rate</span></code> TensorFlow parameter).</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">learning rate at step <code class="docutils literal notranslate"><span class="pre">global_step</span></code>.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="optimizers.lr_policies.transformer_policy">
<code class="descclassname">optimizers.lr_policies.</code><code class="descname">transformer_policy</code><span class="sig-paren">(</span><em>global_step</em>, <em>learning_rate</em>, <em>d_model</em>, <em>warmup_steps</em>, <em>max_lr=None</em>, <em>coefficient=1.0</em>, <em>dtype=tf.float32</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/optimizers/lr_policies.html#transformer_policy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#optimizers.lr_policies.transformer_policy" title="Permalink to this definition">¶</a></dt>
<dd><p>Transformer’s learning rate policy from <a class="reference external" href="https://arxiv.org/pdf/1706.03762.pdf">https://arxiv.org/pdf/1706.03762.pdf</a>
with a hat (max_lr) (also called “noam” learning rate decay scheme).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>global_step</strong> – global step TensorFlow tensor (ignored for this policy).</li>
<li><strong>learning_rate</strong> (<em>float</em>) – initial learning rate to use.</li>
<li><strong>d_model</strong> (<em>int</em>) – model dimensionality.</li>
<li><strong>warmup_steps</strong> (<em>int</em>) – number of warm-up steps.</li>
<li><strong>max_lr</strong> (<em>float</em>) – maximal learning rate, i.e. hat.</li>
<li><strong>coefficient</strong> (<em>float</em>) – optimizer adjustment.
Recommended 0.002 if using “Adam” else 1.0.</li>
<li><strong>dtype</strong> – dtype for this policy.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">learning rate at step <code class="docutils literal notranslate"><span class="pre">global_step</span></code>.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="parts.html" class="btn btn-neutral float-right" title="parts" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="losses.html" class="btn btn-neutral" title="losses" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, NVIDIA.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'0.2',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  <script type="text/javascript">
      jQuery(function () {
          
          SphinxRtdTheme.Navigation.enableSticky();
          
      });
  </script>  
  <style>
    /* Sidebar header (and topbar for mobile) */
    .wy-side-nav-search, .wy-nav-top {
      background: #64d81c;
    }
    .wy-side-nav-search > div.version {
      color: #ffffff;
    }
    .wy-side-nav-search > img {
      max-width: 150px;
    }
    .wy-side-nav-search > a {
      font-size: 23px;
    }
  </style>


</body>
</html>