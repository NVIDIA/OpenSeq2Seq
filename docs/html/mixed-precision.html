

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Mixed precision training &mdash; OpenSeq2Seq 0.2 documentation</title>
  

  
  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  
  
  

  

  
  
    

  

  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/theme_override.css" type="text/css" />
  <link rel="stylesheet" href="_static/theme_override.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="In-depth tutorials" href="in-depth-tutorials.html" />
    <link rel="prev" title="Distributed training" href="distr-training.html" /> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> OpenSeq2Seq
          

          
            
            <img src="_static/logo.png" class="logo" alt="Logo"/>
          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="index.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation instructions</a></li>
<li class="toctree-l1"><a class="reference internal" href="machine-translation.html">Machine Translation</a></li>
<li class="toctree-l1"><a class="reference internal" href="speech-recognition.html">Speech Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="speech-synthesis.html">Speech Synthesis</a></li>
<li class="toctree-l1"><a class="reference internal" href="distr-training.html">Distributed training</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Mixed precision training</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#prerequisites">Prerequisites</a></li>
<li class="toctree-l2"><a class="reference internal" href="#how-to-enable-mixed-precision">How to enable mixed precision</a></li>
<li class="toctree-l2"><a class="reference internal" href="#implementation-details">Implementation details</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#optimizer">Optimizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#regularizers">Regularizers</a></li>
<li class="toctree-l3"><a class="reference internal" href="#automatic-loss-scaling">Automatic Loss Scaling</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="in-depth-tutorials.html">In-depth tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="interactive-infer-demos.html">Interactive Infer Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="api-docs/modules.html">API documentation</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">OpenSeq2Seq</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Mixed precision training</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/mixed-precision.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="mixed-precision-training">
<span id="mixed-precision"></span><h1>Mixed precision training<a class="headerlink" href="#mixed-precision-training" title="Permalink to this headline">¶</a></h1>
<blockquote class="epigraph">
<div><p>Increasing the size of a neural network typically improves accuracy but also
increases the memory and compute requirements for training the model.
We introduce methodology for training deep neural networks using
half-precision floating point numbers, without losing model accuracy or
having to modify hyperparameters. This nearly halves memory requirements
and, on recent GPUs, speeds up arithmetic. …</p>
<p>DNN operations benchmarked with DeepBench on Volta GPU see 2-6x speedups
compared to FP32 implementations if they are limited by memory or arithmetic
bandwidth. Speedups are lower when operations are latency-limited.</p>
<p class="attribution">&mdash;“Mixed Precision Training”, Micikevicius et al, ICLR, 2018 <a class="reference internal" href="#mp-2018" id="id1">[1]</a></p>
</div></blockquote>
<div class="section" id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this headline">¶</a></h2>
<p>Mixed precision training utilizes Tensor Cores introduced in <a class="reference external" href="https://www.nvidia.com/en-us/data-center/volta-gpu-architecture/">NVIDIA Volta GPUs</a>
such as <a class="reference external" href="https://www.nvidia.com/en-us/titan/titan-v/">Titan V</a> and <a class="reference external" href="https://www.nvidia.com/en-us/data-center/tesla/tesla-qualified-servers-catalog/">Tesla V100</a>.
NVIDIA Volta GPUs are also available from <a class="reference external" href="https://aws.amazon.com/blogs/aws/new-amazon-ec2-instances-with-up-to-8-nvidia-tesla-v100-gpus-p3/">AWS on p3.2xlarge, p3.8xlarge, p3.16xlarge instances</a> .</p>
<p>For an optimal mixed precision performance we recommend using NVIDIA’s TensorFlow docker containers (version 18.03 and above)
which can be obtained here: <a class="reference external" href="http://ngc.nvidia.com/">NVIDIA GPU cloud</a> .
Alternatively, you can build TensorFlow yourself with CUDA 9.1 and this <a class="reference external" href="https://github.com/tensorflow/tensorflow/pull/18436">PR</a> included:</p>
</div>
<div class="section" id="how-to-enable-mixed-precision">
<h2>How to enable mixed precision<a class="headerlink" href="#how-to-enable-mixed-precision" title="Permalink to this headline">¶</a></h2>
<p>Enabling mixed precision with existing models in OpenSeq2Seq is simple:
change <code class="docutils literal notranslate"><span class="pre">dtype</span></code> parameter of <code class="docutils literal notranslate"><span class="pre">model_params</span></code> to “mixed”.
You might need to enable loss scaling: either statically, by setting
<code class="docutils literal notranslate"><span class="pre">loss_scale</span></code> parameter inside <code class="docutils literal notranslate"><span class="pre">model_params</span></code> to the desired number, or
you can use dynamic loss scaling by setting <code class="docutils literal notranslate"><span class="pre">automatic_loss_scaling</span></code> parameter
to “Backoff” or “LogMax”:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span> <span class="n">base_params</span> <span class="o">=</span> <span class="p">{</span>
   <span class="o">...</span>
   <span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="s2">&quot;mixed&quot;</span><span class="p">,</span>
   <span class="c1"># enabling static or dynamic loss scaling might improve model convergence</span>

   <span class="c1"># &quot;loss_scale&quot;: 10.0,</span>
   <span class="c1"># &quot;automatic_loss_scaling&quot;: &quot;Backoff&quot;,</span>
   <span class="o">...</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="implementation-details">
<h2>Implementation details<a class="headerlink" href="#implementation-details" title="Permalink to this headline">¶</a></h2>
<p>For mixed precision training we follow an algorithmic recipe from
Micikevicius et al <a class="reference internal" href="#mp-2018" id="id2">[1]</a>. At a high level it can be summarized
as follows:</p>
<ol class="arabic simple">
<li>Maintain and update a float32 master copy of weights (using the float16 copy
for forward and back propagation)</li>
<li>Apply loss scaling while computing gradients</li>
</ol>
<p>It is worth mentioning that (1)-(2) are not always necessary. However, this
method has proven to be robust across a variety of bigger and more complex
models.</p>
<p>Note that while (1) does mean a 50% increase in memory consumption for weights
over a float32 version, in practice, the total memory consumption is often
<em>decreased</em>. This is because activations, activation gradients, and other
intermediate tensors can now be kept in float16. This is especially beneficial
for models with a high degree of parameter sharing or reuse, such as recurrent
models with many timesteps.</p>
<div class="section" id="optimizer">
<h3>Optimizer<a class="headerlink" href="#optimizer" title="Permalink to this headline">¶</a></h3>
<p>Our implementation is different from the one described in
<a class="reference external" href="https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/">NVIDIA documentation</a>:
instead of a custom variable getter, we introduce a wrapper around standard
TensorFlow optimizers. The model is created with float16 data type, so all
variables and gradients are in float16 by default (except for the layers which
are explicitly redefined as float32; for example data layers or operations on
CPU). The wrapper then converts float16 gradients to float32 and submits them
to TensorFlow’s optimizer, which updates the master copy of weights. Updated
weights are converted back to float16, and used by the model in the next
iteration. The <a class="reference internal" href="api-docs/optimizers.html#optimizers.mp_wrapper.MixedPrecisionOptimizerWrapper" title="optimizers.mp_wrapper.MixedPrecisionOptimizerWrapper"><code class="xref py py-class docutils literal notranslate"><span class="pre">MixedPrecisionOptimizerWrapper</span></code></a>
architecture is graphically illustrated below:</p>
<div class="figure align-center" id="id4">
<a class="reference internal image-reference" href="_images/MixedPrecisionOptimizer.png"><img alt="_images/MixedPrecisionOptimizer.png" src="_images/MixedPrecisionOptimizer.png" style="width: 499.5px; height: 161.0px;" /></a>
<p class="caption"><span class="caption-text">“Mixed precision” optimizer wrapper around any TensorFlow optimizer.</span></p>
</div>
</div>
<div class="section" id="regularizers">
<h3>Regularizers<a class="headerlink" href="#regularizers" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="api-docs/optimizers.html#optimizers.mp_wrapper.MixedPrecisionOptimizerWrapper" title="optimizers.mp_wrapper.MixedPrecisionOptimizerWrapper"><code class="xref py py-class docutils literal notranslate"><span class="pre">MixedPrecisionOptimizerWrapper</span></code></a>
ensures that all float16 variables will have
a master copy in float32 and that their gradients will be cast to the full
precision before computing the weight update. While this is enough in most
situations, in some cases it is important to keep the gradient in float32 from
the beginning of the computation. One common case when this is necessary is
weight decay regularization. This regularization results in the following
addition to the usual gradients with respect to the loss:
<span class="math notranslate nohighlight">\(\frac{\partial L}{\partial w} \mathrel{+}= 2\lambda w\)</span>,
where <span class="math notranslate nohighlight">\(\lambda\)</span> is usually on the order of
<span class="math notranslate nohighlight">\(\left[10^{-5}, 10^{-3}\right]\)</span>.
Given that the weights are commonly initialized with small values, multiplying
them with weight decay coefficient $lambda$ can result in numerical underflow.
To overcome this problem we implemented the following design principles. First,
all regularizers should be defined on the variable creation level by passing
regularizer function as a regularizer parameter to the <code class="docutils literal notranslate"><span class="pre">tf.get_variable</span></code>
function or <code class="docutils literal notranslate"><span class="pre">tf.layers</span></code> objects (this is a recommended way to do it in
TensorFlow). Second, the regularizer function should be wrapped with
<a class="reference internal" href="api-docs/optimizers.html#optimizers.mp_wrapper.mp_regularizer_wrapper" title="optimizers.mp_wrapper.mp_regularizer_wrapper"><code class="xref py py-func docutils literal notranslate"><span class="pre">mp_regularizer_wrapper</span></code></a>
function which will do two things. First, it
will add weights and the user-provided regularization function to the TensorFlow
collection. And second, it will disable the underlying regularization function
by returning None (only if the weights are in float16, otherwise it will not
introduce any additional behavior). The created collection will later be
retrieved by <code class="docutils literal notranslate"><span class="pre">MixedPrecisionOptimizerWrapper</span></code> and the corresponding
functions will be applied to the float32 copy of the weights ensuring that their
gradients always stay in the full precision. Since this regularization will not
be a part of the loss computation graph, we explicitly call <code class="docutils literal notranslate"><span class="pre">tf.gradients</span></code>
and add the result to the gradients passed in the <code class="docutils literal notranslate"><span class="pre">compute_gradients</span></code>
function of the optimizer.</p>
</div>
<div class="section" id="automatic-loss-scaling">
<h3>Automatic Loss Scaling<a class="headerlink" href="#automatic-loss-scaling" title="Permalink to this headline">¶</a></h3>
<p>The mixed precision training approach of Micikevicius et al <a class="reference internal" href="#mp-2018" id="id3">[1]</a>
suggests that the user
set a <em>loss scale</em> hyperparameter to adjust the dynamic range of backpropagation
to match the dynamic range of float16. OpenSeq2Seq implements an extension to
the mixed precision recipe that we call <em>automatic loss scaling</em>. The optimizer
inspects the parameter gradients at each iteration and uses their values to
select the loss scale for the <em>next</em> iteration. As a result, the user does not
have to select the loss-scale value manually.
Concretely, OpenSeq2Seq has support for two automatic loss scaling algorithms,
<em>Backoff</em> and <em>LogNormal</em> scaling.</p>
<ul class="simple">
<li><em>Backoff</em> scaling begins with a large loss scale and checks for overflow in
the parameter gradients at the end of each iteration. Whenever there is an
overflow, the loss scale decreases by a constant factor (default is 2) and the
optimizer will skip the update. Furthermore, if there has been no overflow for
a period of time, the loss scale increases by a constant factor (defaults are
2000 iterations and 2, respectively). These two rules together ensure both
that the loss scale is as large as possible and also that it can adjust to
shifting dynamic range during training.</li>
<li><em>LogNormal</em> scaling uses gradient statistics, rather than the presence of
overflow, to set the loss scale. It keeps a running estimate of the mean and
variance of the inter-iteration maximum absolute value of the parameter
gradients. It models the inter-iteration maximum as log-normally distributed
(hence the name), and then chooses the loss scale for the next iteration s.t.
the probability of the maximum overflowing float16 is less than some constant
(default is 0.001). In the rare event of an overflow, the optimizer skips the
update.</li>
</ul>
<p id="bibtex-bibliography-mixed-precision-0"><table class="docutils citation" frame="void" id="mp-2018" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><em>(<a class="fn-backref" href="#id1">1</a>, <a class="fn-backref" href="#id2">2</a>, <a class="fn-backref" href="#id3">3</a>)</em> Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaev, Ganesh Venkatesh, and others. Mixed precision training. <em>arXiv preprint arXiv:1710.03740</em>, 2017.</td></tr>
</tbody>
</table>
</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="in-depth-tutorials.html" class="btn btn-neutral float-right" title="In-depth tutorials" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="distr-training.html" class="btn btn-neutral" title="Distributed training" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, NVIDIA.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.2',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  <script type="text/javascript">
      jQuery(function () {
          
          SphinxRtdTheme.Navigation.enableSticky();
          
      });
  </script>  
  <style>
    /* Sidebar header (and topbar for mobile) */
    .wy-side-nav-search, .wy-nav-top {
      background: #64d81c;
    }
    .wy-side-nav-search > div.version {
      color: #ffffff;
    }
    .wy-side-nav-search > img {
      max-width: 150px;
    }
    .wy-side-nav-search > a {
      font-size: 23px;
    }
  </style>


</body>
</html>