

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Language Model &mdash; OpenSeq2Seq 0.2 documentation</title>
  

  
  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  
  
  

  

  
  
    

  

  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/theme_override.css" type="text/css" />
  <link rel="stylesheet" href="_static/theme_override.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="LSTMLM" href="language-model/lstmlm.html" />
    <link rel="prev" title="WaveNet" href="speech-synthesis/wavenet.html" /> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> OpenSeq2Seq
          

          
            
            <img src="_static/logo.png" class="logo" alt="Logo"/>
          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="index.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation instructions</a></li>
<li class="toctree-l1"><a class="reference internal" href="machine-translation.html">Machine Translation</a></li>
<li class="toctree-l1"><a class="reference internal" href="speech-recognition.html">Speech Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="speech-synthesis.html">Speech Synthesis</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Language Model</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#models">Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="language-model/lstmlm.html">LSTMLM</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#getting-started">Getting started</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#get-data">Get data</a></li>
<li class="toctree-l3"><a class="reference internal" href="#training">Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="#inference">Inference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="sentiment-analysis.html">Sentiment Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="distr-training.html">Multi-GPU and Distributed Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="mixed-precision.html">Mixed precision training</a></li>
<li class="toctree-l1"><a class="reference internal" href="in-depth-tutorials.html">In-depth tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="interactive-infer-demos.html">Interactive Infer Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="api-docs/modules.html">API documentation</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">OpenSeq2Seq</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Language Model</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/language-model.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="language-model">
<span id="id1"></span><h1>Language Model<a class="headerlink" href="#language-model" title="Permalink to this headline">¶</a></h1>
<div class="section" id="models">
<h2>Models<a class="headerlink" href="#models" title="Permalink to this headline">¶</a></h2>
<p>Currently we support the following models:</p>
<table border="1" class="colwidths-given docutils">
<colgroup>
<col width="25%" />
<col width="50%" />
<col width="25%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Model description</th>
<th class="head">Config file</th>
<th class="head">Checkpoint</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><span class="xref std std-doc">LSTM with WikiText-2</span></td>
<td><a class="reference external" href="https://github.com/NVIDIA/OpenSeq2Seq/blob/master/example_configs/lm/lstm-wkt2-fp32.py">lstm-wkt2-fp32.py</a></td>
<td><a class="reference external" href="https://drive.google.com/a/nvidia.com/file/d/1uP-zALrSDb_dz7r7OUqEOakEbSIkxGEp/view?usp=sharing">Perplexity=89.9</a></td>
</tr>
<tr class="row-odd"><td><span class="xref std std-doc">LSTM with WikiText-103</span></td>
<td><a class="reference external" href="https://github.com/NVIDIA/OpenSeq2Seq/blob/master/example_configs/lm/lstm-wkt103-mixed.py">lstm-wkt103-mixed.py</a></td>
<td><a class="reference external" href="https://drive.google.com/a/nvidia.com/file/d/1XC4oN7PXwJwR0KFgljsJQ83CDS09LnIb/view?usp=sharing">Perplexity=48.6</a></td>
</tr>
</tbody>
</table>
<p>The model specification and training parameters can be found in the corresponding config file.</p>
<div class="toctree-wrapper compound">
</div>
</div>
<div class="section" id="getting-started">
<h2>Getting started<a class="headerlink" href="#getting-started" title="Permalink to this headline">¶</a></h2>
<p>The current LSTM language model implementation supports the
<a class="reference external" href="https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/">WikiText-2 and WikiText-103</a> datasets. For more details about the model including hyperparameters and tips, please see
<a class="reference internal" href="language-model/lstmlm.html"><span class="doc">LSTM Language Model</span></a>.</p>
<div class="section" id="get-data">
<h3>Get data<a class="headerlink" href="#get-data" title="Permalink to this headline">¶</a></h3>
<p>The WkiText-103 dataset, developed by Salesforce, contains over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. It has 267,340 unique tokens that appear at least 3 times in the dataset. Since it has full-length Wikipedia articles, the dataset is well-suited for tasks that can benefit of long term dependencies, such as language modeling.</p>
<p>The WikiText-2 dataset is a small version of the WikiText-103 dataset as it contains only 2 million tokens. This small dataset is suitable for testing your language model.</p>
<p>The WikiText datasets are available in both word-level (with minor preprocessing and rare tokens being replaced with &lt;UNK&gt;) and the raw character level. OpenSeq2Seq’s WKTDataLayer is equipped to deal with both versions, but we recommend that you use the raw dataset.</p>
<p>You can download the datasets <cite>here &lt;https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/&gt;</cite>, extract them to the location of your choice. The dataset should contain of 3 files for train, validation, and test. Don’t forget to update the <code class="docutils literal notranslate"><span class="pre">data_root</span></code> parameter in your config file to point to the location of your dataset.</p>
<p>WKTDataLayer does the necessary pre-processing to make the WikiText datasets ready to be fed into the model. We use the <code class="docutils literal notranslate"><span class="pre">word_token</span></code> method available in the <code class="docutils literal notranslate"><span class="pre">nltk</span></code> package.</p>
<p>You can pre-process the data for your language model in any way you deem fit. However, if you want to use your trained language model for other tasks such as sentiment analysis, make sure that the dataset used for your language model and the dataset used for the sentiment analysis model have similar pre-processing and share the same vocabulary.</p>
</div>
<div class="section" id="training">
<h3>Training<a class="headerlink" href="#training" title="Permalink to this headline">¶</a></h3>
<p>Next let’s create a simple LSTM language model by defining a config file for it or using one of the config files defined in <code class="docutils literal notranslate"><span class="pre">example_configs/lstmlm</span></code>.</p>
<ul class="simple">
<li>change <code class="docutils literal notranslate"><span class="pre">data_root</span></code> to point to the directory containing the raw dataset used to train your language model, for example, your WikiText dataset downloaded above.</li>
<li>change <code class="docutils literal notranslate"><span class="pre">processed_data_folder</span></code> to point to the location where you want to store the processed dataset. If the dataset has been pre-procesed before, the data layer can just load the data from this location.</li>
<li>update other hyper parameters such as number of layers, number of hidden units, cell type, loss function, learning rate, optimizer, etc. to meet your needs.</li>
<li>choose <code class="docutils literal notranslate"><span class="pre">dtype</span></code> to be <code class="docutils literal notranslate"><span class="pre">&quot;mixed&quot;</span></code> if you want to use mixed-precision training, or <code class="docutils literal notranslate"><span class="pre">tf.float32</span></code> to train only in FP32.</li>
</ul>
<p>For example, your config file is <code class="docutils literal notranslate"><span class="pre">lstm-wkt103-mixed.py</span></code>. To train without Horovod, update <code class="docutils literal notranslate"><span class="pre">use_Horovod</span></code> to False in the config file and run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">run</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">config_file</span><span class="o">=</span><span class="n">example_configs</span><span class="o">/</span><span class="n">lstmlm</span><span class="o">/</span><span class="n">lstm</span><span class="o">-</span><span class="n">wkt103</span><span class="o">-</span><span class="n">mixed</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">mode</span><span class="o">=</span><span class="n">train_eval</span> <span class="o">--</span><span class="n">enable_logs</span>
</pre></div>
</div>
<p>When training with Horovod, use the following command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">mpiexec</span> <span class="o">--</span><span class="n">allow</span><span class="o">-</span><span class="n">run</span><span class="o">-</span><span class="k">as</span><span class="o">-</span><span class="n">root</span> <span class="o">-</span><span class="n">np</span> <span class="o">&lt;</span><span class="n">num_gpus</span><span class="o">&gt;</span> <span class="n">python</span> <span class="n">run</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">config_file</span><span class="o">=</span><span class="n">example_configs</span><span class="o">/</span><span class="n">lstmlm</span><span class="o">/</span><span class="n">lstm</span><span class="o">-</span><span class="n">wkt103</span><span class="o">-</span><span class="n">mixed</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">mode</span><span class="o">=</span><span class="n">train_eval</span> <span class="o">--</span><span class="n">use_horovod</span><span class="o">=</span><span class="kc">True</span> <span class="o">--</span><span class="n">enable_logs</span>
</pre></div>
</div>
<p>Some things to keep in mind:</p>
<ul class="simple">
<li>Don’t forget to update <code class="docutils literal notranslate"><span class="pre">num_gpus</span></code> to the number of GPUs you want to use.</li>
<li>If the vocabulary is large (the word-level vocabulary for WikiText-103 is 267,000+), you might want to use <code class="docutils literal notranslate"><span class="pre">BasicSampledSequenceLoss</span></code>, which uses sampled softnax, instead of <code class="docutils literal notranslate"><span class="pre">BasicSequenceLoss</span></code>, which uses full softmax.</li>
<li>If your GPUs still run out of memory, reduce the <code class="docutils literal notranslate"><span class="pre">batch_size_per_gpu</span></code></li>
</ul>
<p>parameter.</p>
</div>
<div class="section" id="inference">
<h3>Inference<a class="headerlink" href="#inference" title="Permalink to this headline">¶</a></h3>
<p>Even if your training is done using sampled softmax, evaluation and text generation will always be done using full softmax. Running in the mode <code class="docutils literal notranslate"><span class="pre">eval</span></code> will evaluate your model on the evaluation set:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">run</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">config_file</span><span class="o">=</span><span class="n">example_configs</span><span class="o">/</span><span class="n">lstmlm</span><span class="o">/</span><span class="n">lstm</span><span class="o">-</span><span class="n">wkt103</span><span class="o">-</span><span class="n">mixed</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">mode</span><span class="o">=</span><span class="nb">eval</span> <span class="o">--</span><span class="n">enable_logs</span>
</pre></div>
</div>
<p>Running in the mode <code class="docutils literal notranslate"><span class="pre">infer</span></code> will generate text from the seed tokens, defined in the config file under the parameter name <code class="docutils literal notranslate"><span class="pre">seed_tokens</span></code>, each seed token should be separated by space. [TODO: make <code class="docutils literal notranslate"><span class="pre">seed_tokens</span></code> take a list of strings instead]:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">run</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">config_file</span><span class="o">=</span><span class="n">example_configs</span><span class="o">/</span><span class="n">lstmlm</span><span class="o">/</span><span class="n">lstm</span><span class="o">-</span><span class="n">wkt103</span><span class="o">-</span><span class="n">mixed</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">mode</span><span class="o">=</span><span class="n">infer</span> <span class="o">--</span><span class="n">enable_logs</span>
</pre></div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="language-model/lstmlm.html" class="btn btn-neutral float-right" title="LSTMLM" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="speech-synthesis/wavenet.html" class="btn btn-neutral" title="WaveNet" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, NVIDIA.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.2',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  <script type="text/javascript">
      jQuery(function () {
          
          SphinxRtdTheme.Navigation.enableSticky();
          
      });
  </script>  
  <style>
    /* Sidebar header (and topbar for mobile) */
    .wy-side-nav-search, .wy-nav-top {
      background: #64d81c;
    }
    .wy-side-nav-search > div.version {
      color: #ffffff;
    }
    .wy-side-nav-search > img {
      max-width: 150px;
    }
    .wy-side-nav-search > a {
      font-size: 23px;
    }
  </style>


</body>
</html>